{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to demonstrate summe label issues and validation split consistency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Model import model_dict,params_dict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from Utils import *\n",
    "import torch.optim as optim\n",
    "from Data import VideoData\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "import argparse\n",
    "import h5py\n",
    "results_dir = \"Results\"\n",
    "weights_path = 'weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config_path,save_path = 'weights'):\n",
    "    with open(config_path,'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "    \n",
    "    assert config['Model'] in model_dict.keys(), \"Model is not available, modify dictionary to include them or check spelling\"\n",
    "    dataset_name = config['split'].split(\"_\")[0]\n",
    "    split_string = config['split'].strip(dataset_name).strip('.json')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    modelclass = model_dict[config['Model']]\n",
    "    criterion = loss_dict[config['loss_function']]()\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    feature_extractor = config['feature_extractor']\n",
    "    save_name = f'{feature_extractor}_{dataset_name}{split_string}'\n",
    "    if not os.path.exists(os.path.join(save_path,save_name,dataset_name,config['Model'] )):\n",
    "        os.makedirs(os.path.join(save_path,save_name,dataset_name,config['Model'] ))\n",
    "\n",
    "\n",
    "    save_path = os.path.join(save_path,save_name,dataset_name,config['Model'] )\n",
    "    print(save_name)\n",
    "\n",
    "    params = params_dict[config['Model']][config['feature_extractor']]\n",
    "\n",
    "    if config['data_aug'] :\n",
    "        #data_augmentations  = [shuffle_dict[data_aug](**config['data_aug'][data_aug]) for data_aug in config['data_aug']]\n",
    "        pass\n",
    "    else:\n",
    "        data_augmentations = []\n",
    "    splits = config['total_splits'] if 'total_splits' in config.keys() else 5\n",
    "    #dataset = h5py.File(config['datapath']+'.h5')\n",
    "    dataset = h5py.File(os.path.join('Data',config['feature_extractor'],f'{config[\"feature_extractor\"]}_{dataset_name}.h5'))\n",
    "    print(params)\n",
    "    for split in range(splits):\n",
    "        print(f\"Running Split:  {split+1}  for model: {config['Model']}\")\n",
    "        model = modelclass(**params)\n",
    "        batchloader = VideoData('train',config['split'],split,transforms=data_augmentations,feature_extractor=feature_extractor,trainval=True)\n",
    "        batchloader = DataLoader(batchloader,batch_size=1,shuffle=True)\n",
    "        testdata = VideoData('test',config['split'],split,feature_extractor=feature_extractor,trainval=True)\n",
    "        testloader = DataLoader(testdata,batch_size=1,shuffle=False)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"],weight_decay=config['reg'])\n",
    "        best_f1_score = -float('inf')\n",
    "        best_correlation = -float('inf')\n",
    "        model.to(device)\n",
    "        if 'gradnorm_clip' in config:\n",
    "            gradnorm_clip = config['gradnorm_clip']\n",
    "        else:\n",
    "            gradnorm_clip = 3\n",
    "        # Make the directory for the split if it doesn't exist \n",
    "        if not os.path.exists(os.path.join(save_path,f'split_{split+1}')):\n",
    "            os.mkdir(os.path.join(save_path,f'split_{split+1}'))\n",
    "        save_path_split = os.path.join(save_path,f'split_{split+1}')\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            total_samples = 0\n",
    "\n",
    "            for data in tqdm(batchloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", ncols=100):\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                labels-=labels.min()\n",
    "                labels/=labels.max()\n",
    "                outputs = model(inputs)\n",
    "                if len(outputs.shape)>2:\n",
    "                    outputs = outputs.squeeze(-1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradnorm_clip)\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                total_samples+=1\n",
    "            epoch_loss = running_loss / len(batchloader)\n",
    "            print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "            model.eval()\n",
    "            test_datapoints = []\n",
    "            test_names = []\n",
    "            \n",
    "# Adding the correlation scores to have the picks from the datapoints \n",
    "            print(f\"Compute F1 and Correlation for epoch: {epoch+1}\")\n",
    "            for inputs_t,names in tqdm(testloader,ncols=len(testdata)):\n",
    "                with torch.no_grad():\n",
    "                    importance_scores = model(inputs_t.to(device))\n",
    "                importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "                test_datapoints.append(importance_scores)\n",
    "                test_names.append(names[0])\n",
    "            all_scores = eval_summary(test_datapoints,dataset,test_names,dataset_name)\n",
    "    \n",
    "            correlation_dict = evaluate_correlation(test_datapoints ,dataset,test_names,dataset_name)\n",
    "            \n",
    "            if correlation_dict['Average_Kendall']> best_correlation:    \n",
    "                print(f\"Saving epoch {epoch+1}\")\n",
    "                best_correlation = correlation_dict['Average_Kendall']\n",
    "                print(f\"Best Correlation Score:  {epoch+1}: {correlation_dict['Average_Kendall']} \")  # CHange this here \n",
    "                torch.save(model.state_dict(), os.path.join(save_path_split,\"best_run_corr\" + \".pth\")) \n",
    "\n",
    "            if np.mean(all_scores).item() > best_f1_score:\n",
    "                best_f1_score = np.mean(all_scores).item()\n",
    "                print(f\"Best F1 Score:  {epoch+1}: {best_f1_score} \")\n",
    "                torch.save(model.state_dict(), os.path.join(save_path_split,\"best_run_f1\" + \".pth\"))\n",
    "\n",
    "        print(f'Best F1 score for split {split+1}: {best_f1_score} ')\n",
    "        print(f'Best Correlation for split {split+1}: {best_correlation} ')\n",
    "    print('Completed Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(config_path,delete_weights=True):\n",
    "    with open(config_path,'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "    if delete_weights==True:\n",
    "        print(\"IMPORTANT NOTE: WEIGHTS ARE DELETED, RERUN TRAINING TO GET THEM BACK\")\n",
    "    assert config['Model_params']['Model'] in model_dict.keys(), \"Model is not available, modify dictionary to include them or check spelling\"\n",
    "    \n",
    "    #TODO perhaps make this a bit more flexible with the names\n",
    "\n",
    "    dataset_name = config['split'].split(\"_\")[0]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # save_path = os.path.join(args.save_path,config['save_name'],dataset_name,config['Model_params']['Model'] )   \n",
    "    feature_extractor = config['feature_extractor']    \n",
    "    modelclass = model_dict[config['Model_params']['Model']]\n",
    "    \n",
    "    if \"Params\" in list(config['Model_params'].keys()):\n",
    "        params = config['Model_params']['Params']\n",
    "    else:\n",
    "        params = {}   # Running checks and creating results for each experiment\n",
    "        #Save it as results/experiment/model\n",
    "    \n",
    "    #---------------------------------------------------- BEST Correlation runs-------------------------------------------------------------------------\n",
    "    \n",
    "    if not os.path.exists(os.path.join(results_dir,config['save_name'],'best_corr',dataset_name,config['Model_params']['Model'])):\n",
    "        os.makedirs(os.path.join(results_dir,config['save_name'],'best_corr',dataset_name,config['Model_params']['Model']))\n",
    "    result_dir = os.path.join(results_dir,config['save_name'],'best_corr',dataset_name,config['Model_params']['Model'])\n",
    "    # Check if the F1 score and the correlation directories for the results exist\n",
    "    if not os.path.exists(os.path.join(result_dir,'F1')):\n",
    "        os.mkdir(os.path.join(result_dir,'F1'))\n",
    "    if not os.path.exists(os.path.join(result_dir,'Correlation')):\n",
    "        os.mkdir(os.path.join(result_dir,'Correlation'))\n",
    "    if not os.path.exists(os.path.join(result_dir,'Output')):\n",
    "        os.mkdir(os.path.join(result_dir,'Output'))\n",
    "    result_f1_dir = os.path.join(result_dir,'F1')\n",
    "    result_corr_dir = os.path.join(result_dir,'Correlation')\n",
    "    result_f1_json = os.path.join(result_dir,'F1','results.json')\n",
    "    result_corr_json = os.path.join(result_dir,'Correlation','results.json')\n",
    "    result_out_json = os.path.join(result_dir,'Output','outputs.json')\n",
    "\n",
    "\n",
    "\n",
    "    splits = 5\n",
    "    # Loading the weights:\n",
    "        # Loading the weights as : \"weights/experiment/model\"\n",
    "    weight_path = os.path.join(weights_path,config['save_name'],dataset_name,config['Model_params']['Model'])\n",
    "    print(os.path.join(weights_path,config['save_name'],dataset_name,config['Model_params']['Model']))\n",
    "    assert os.path.exists(os.path.join(weights_path,config['save_name'],dataset_name,config['Model_params']['Model'])), \"Model weights do not exist or pathing is incorrect, check path\"\n",
    "    dataset = h5py.File(os.path.join(config['datapath']+'.h5'))\n",
    "    \n",
    "    # Needs to do: Inference -> Correlation -> Post Process -> F1 -> save results in results dir\n",
    "    all_splits_f1_scores = {}\n",
    "    all_splits_correlations = {}\n",
    "    output_dict ={}\n",
    "    for split in range(splits):\n",
    "        model = modelclass(**params)\n",
    "        testdata = VideoData('test',config['split'],split,feature_extractor=feature_extractor)\n",
    "        testloader = DataLoader(testdata,batch_size=1,shuffle=False)\n",
    "        weight_path_split = os.path.join(weight_path,f\"split_{split+1}\",'best_run_corr.pth')\n",
    "        model.load_state_dict(torch.load(weight_path_split,map_location=device))\n",
    "        name_list = []\n",
    "        output_list = []\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        for inputs,names in tqdm(testloader,ncols=100):\n",
    "            inputs = inputs.to(device)\n",
    "            with torch.no_grad():\n",
    "                importance_scores = model(inputs)\n",
    "                if len(importance_scores.shape)>2:\n",
    "                    importance_scores = importance_scores.squeeze(-1)\n",
    "            importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "            output_list.append(importance_scores)\n",
    "            name_list.append(names[0])\n",
    "            output_dict[str(names[0])] = importance_scores\n",
    "        result_f1_dict = generate_f1_results(output_list,dataset,name_list,dataset_name) # This needs to be dumped into a JSON for each split\n",
    "        correlation_dict = evaluate_correlation(output_list,dataset,name_list,dataset_name) # This as well\n",
    "        \n",
    "        # Saving the split in the respective directories\n",
    "\n",
    "        split_save_f1_name = os.path.join(result_f1_dir,f\"split_{split+1}.json\")\n",
    "        split_save_corr_name = os.path.join(result_corr_dir,f\"split_{split+1}.json\")\n",
    "        split_save_f1_noname = os.path.join(result_f1_dir,f\"split_{split+1}_noname.json\")\n",
    "        split_save_corr_noname = os.path.join(result_corr_dir,f\"split_{split+1}_noname.json\")\n",
    "        \n",
    "        \n",
    "        all_splits_f1_scores[f'split_{split+1}'] = result_f1_dict['Average F1']\n",
    "        all_splits_correlations[f'split_{split+1}'] = {}\n",
    "        all_splits_correlations[f'split_{split+1}']['Kendall']= correlation_dict['Average_Kendall']\n",
    "        all_splits_correlations[f'split_{split+1}']['Spearman']= correlation_dict['Average_Spearman']\n",
    "       \n",
    "        # Pop the average keys and then use it to save the split with name of the original\n",
    "        result_f1_dict.pop('Average F1')\n",
    "        correlation_dict.pop('Average_Kendall')\n",
    "        correlation_dict.pop('Average_Spearman')\n",
    "        #TODO: Remember to fix the key issue from before when you are evaluating the correlation and F1\n",
    "        with open(split_save_f1_noname,'w') as json_file:\n",
    "            json.dump(result_f1_dict,json_file,indent=4)\n",
    "        with open(split_save_corr_noname,'w') as json_file:\n",
    "            json.dump(correlation_dict,json_file,indent=4)\n",
    "        result_f1_dict,correlation_dict = change_key_names(result_f1_dict,correlation_dict,dataset_name)\n",
    "\n",
    "        # TODO DUMP THE JSONS\n",
    "        with open(split_save_f1_name,'w') as json_file:\n",
    "            json.dump(result_f1_dict,json_file,indent=4)\n",
    "        with open(split_save_corr_name,'w') as json_file:\n",
    "            json.dump(correlation_dict,json_file,indent=4)\n",
    "        \n",
    "        if delete_weights==True:\n",
    "            os.remove(os.path.join(weight_path,f\"split_{split+1}\",'best_run_corr.pth'))\n",
    "\n",
    "        \n",
    "    print(\"----------------------------------------------- Final set of results from the experiments for the best correlation weights ------------------------------------------------------------------------------------------------\")\n",
    "    print(all_splits_correlations)\n",
    "    result_f1_dict_final,correlation_dict_final = compute_average_results(all_splits_f1_scores,all_splits_correlations)\n",
    "    print()\n",
    "\n",
    "    with open(result_f1_json,'w') as json_file:\n",
    "        json.dump(result_f1_dict_final,json_file,indent = 4 )\n",
    "    with open(result_corr_json,'w') as json_file:\n",
    "        json.dump(correlation_dict_final,json_file,indent = 4 )\n",
    "    with open(result_out_json,'w') as json_file:\n",
    "        json.dump(output_dict,json_file,indent = 4 )\n",
    "   \n",
    "\n",
    "#------------------------------------------Best F1 score runs---------------------------------------------------------\n",
    "    if not os.path.exists(os.path.join(results_dir,config['save_name'],'best_f1',dataset_name,config['Model_params']['Model'])):\n",
    "        os.makedirs(os.path.join(results_dir,config['save_name'],'best_f1',dataset_name,config['Model_params']['Model']))\n",
    "    result_dir = os.path.join(results_dir,config['save_name'],'best_f1',dataset_name,config['Model_params']['Model'])\n",
    "    # Check if the F1 score and the correlation directories for the results exist\n",
    "    if not os.path.exists(os.path.join(result_dir,'F1')):\n",
    "        os.mkdir(os.path.join(result_dir,'F1'))\n",
    "    if not os.path.exists(os.path.join(result_dir,'Correlation')):\n",
    "        os.mkdir(os.path.join(result_dir,'Correlation'))\n",
    "    if not os.path.exists(os.path.join(result_dir,'Output')):\n",
    "        os.mkdir(os.path.join(result_dir,'Output'))\n",
    "    result_f1_dir = os.path.join(result_dir,'F1')\n",
    "    result_corr_dir = os.path.join(result_dir,'Correlation')\n",
    "    result_f1_json = os.path.join(result_dir,'F1','results.json')\n",
    "    result_corr_json = os.path.join(result_dir,'Correlation','results.json')\n",
    "    result_out_json = os.path.join(result_dir,'Output','outputs.json')\n",
    "\n",
    "\n",
    "\n",
    "    splits = 5\n",
    "    # Loading the weights:\n",
    "        # Loading the weights as : \"weights/experiment/model\"\n",
    "    weight_path = os.path.join(weights_path,config['save_name'],dataset_name,config['Model_params']['Model'])\n",
    "    assert os.path.exists(os.path.join(weights_path,config['save_name'],dataset_name,config['Model_params']['Model'])), \"Model weights do not exist or pathing is incorrect, check path\"\n",
    "    dataset = h5py.File(config['datapath']+'.h5')\n",
    "    \n",
    "    # Needs to do: Inference -> Correlation -> Post Process -> F1 -> save results in results dir\n",
    "    output_dict ={}\n",
    "    all_splits_f1_scores = {}\n",
    "    all_splits_correlations = {}\n",
    "    for split in range(splits):\n",
    "        model = modelclass(**params)\n",
    "        testdata = VideoData('test',config['split'],split,feature_extractor=feature_extractor)\n",
    "        testloader = DataLoader(testdata,batch_size=1,shuffle=False)\n",
    "        weight_path_split = os.path.join(weight_path,f\"split_{split+1}\",'best_run_f1.pth')\n",
    "        model.load_state_dict(torch.load(weight_path_split,map_location=device))\n",
    "        \n",
    "        name_list = []\n",
    "        output_list = []\n",
    "        model.eval()\n",
    "        model.to(device)\n",
    "        print('here')\n",
    "        for inputs,names in tqdm(testloader,ncols=100):\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            with torch.no_grad():\n",
    "                importance_scores = model(inputs)\n",
    "                if len(importance_scores.shape)>2:\n",
    "                    importance_scores = importance_scores.squeeze(-1)\n",
    "            importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "            output_list.append(importance_scores)\n",
    "            name_list.append(names[0])\n",
    "            output_dict[str(names[0])] = importance_scores\n",
    "        \n",
    "        result_f1_dict = generate_f1_results(output_list,dataset,name_list,dataset_name) # This needs to be dumped into a JSON for each split\n",
    "        correlation_dict = evaluate_correlation(output_list,dataset,name_list,dataset_name) # This as well\n",
    "        \n",
    "        # Saving the split in the respective directories\n",
    "\n",
    "        split_save_f1_name = os.path.join(result_f1_dir,f\"split_{split+1}.json\")\n",
    "        split_save_corr_name = os.path.join(result_corr_dir,f\"split_{split+1}.json\")\n",
    "        split_save_f1_noname = os.path.join(result_f1_dir,f\"split_{split+1}_noname.json\")\n",
    "        split_save_corr_noname = os.path.join(result_corr_dir,f\"split_{split+1}_noname.json\")\n",
    "        \n",
    "        \n",
    "        all_splits_f1_scores[f'split_{split+1}'] = result_f1_dict['Average F1']\n",
    "        all_splits_correlations[f'split_{split+1}'] = {}\n",
    "        all_splits_correlations[f'split_{split+1}']['Kendall']= correlation_dict['Average_Kendall']\n",
    "        all_splits_correlations[f'split_{split+1}']['Spearman']= correlation_dict['Average_Spearman']\n",
    "       \n",
    "        # Pop the average keys and then use it to save the split with name of the original\n",
    "\n",
    "        result_f1_dict.pop('Average F1')\n",
    "        correlation_dict.pop('Average_Kendall')\n",
    "        correlation_dict.pop('Average_Spearman')\n",
    "\n",
    "        #TODO: Remember to fix the key issue from before when you are evaluating the correlation and F1\n",
    "        with open(split_save_f1_noname,'w') as json_file:\n",
    "            json.dump(result_f1_dict,json_file,indent=4)\n",
    "        with open(split_save_corr_noname,'w') as json_file:\n",
    "            json.dump(correlation_dict,json_file,indent=4)\n",
    "        result_f1_dict,correlation_dict = change_key_names(result_f1_dict,correlation_dict,dataset_name)\n",
    "\n",
    "        # TODO DUMP THE JSONS\n",
    "        with open(split_save_f1_name,'w') as json_file:\n",
    "            json.dump(result_f1_dict,json_file,indent=4)\n",
    "        with open(split_save_corr_name,'w') as json_file:\n",
    "            json.dump(correlation_dict,json_file,indent=4)\n",
    "        if delete_weights==True:\n",
    "            os.remove(os.path.join(weight_path,f\"split_{split+1}\",'best_run_f1.pth'))\n",
    "\n",
    "        \n",
    "    print(\"----------------------------------------------- Final set of results from the experiments for the best f1 score weights ------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    print(all_splits_correlations)\n",
    "    result_f1_dict_final,correlation_dict_final = compute_average_results(all_splits_f1_scores,all_splits_correlations)\n",
    "\n",
    "\n",
    "    with open(result_f1_json,'w') as json_file:\n",
    "        json.dump(result_f1_dict_final,json_file,indent = 4 )\n",
    "    with open(result_corr_json,'w') as json_file:\n",
    "        json.dump(correlation_dict_final,json_file,indent = 4 )\n",
    "        \n",
    "    with open(result_out_json,'w') as json_file:\n",
    "        json.dump(output_dict,json_file,indent = 4 )\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video-learning",
   "language": "python",
   "name": "video-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
