{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to demonstrate evaluation divergence of video summarization models \n",
    "\n",
    "This notebook demonstrates the differences in the each of the evaluation protocols followed in previous research. \n",
    "\n",
    "For this notebook, I'd like to credit the following repositories \n",
    "\n",
    "1. [DSNet](https://github.com/li-plus/DSNet)\n",
    "2. [CSTASUM](https://github.com/thswodnjs3/CSTA)\n",
    "3. [MSVA](https://github.com/TIBHannover/MSVA/tree/master)\n",
    "4. [PGLSUM](https://github.com/e-apostolidis/PGL-SUM/tree/master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions and model loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np \n",
    "import json \n",
    "from Utils import *\n",
    "from Model import model_dict,params_dict\n",
    "import os \n",
    "import torch\n",
    "from Data import VideoData\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "seeds = [12412,31235,123123,53216,123151] # Set the seeds to ensure the results are consistent NOTE: the results are within margin of error, but not exactly obtained on different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Should do comparisons of any pred with avg/user annotations \n",
    "def correlation_single_pred(score,video_name,dataset,dataset_name='tvsum',downsample_gt=True):\n",
    "    \"This compares the scores with a downsampled version of the ground truth, Scenario 1\"\n",
    "    kendall_spearman_scores = []\n",
    "    if dataset_name==\"tvsum\":\n",
    "        data = load_tvsum_mat('Utils//ydata-tvsum50.mat')\n",
    "        video_number = int(video_name.split('_')[1])\n",
    "        all_user_summary = data[video_number-1]['user_anno'].T\n",
    "        pick = dataset[video_name]['picks']\n",
    "        all_correlations_tau = []\n",
    "        all_correlations_spearman = []\n",
    "        for user_summary in all_user_summary:\n",
    "            if downsample_gt:\n",
    "                down_sampled_summary = (user_summary/user_summary.max())[pick] # Change this to take the picks from which a certain frame was sampled from\n",
    "            else:\n",
    "                down_sampled_summary = (user_summary/user_summary.max())\n",
    "        \n",
    "            correlation_tau = kendalltau(-rankdata(down_sampled_summary),-rankdata(score))[0]\n",
    "            correlation_spear = spearmanr(down_sampled_summary,score)[0]\n",
    "            all_correlations_tau.append(correlation_tau)\n",
    "            all_correlations_spearman.append(correlation_spear)\n",
    "        kendall_spearman_scores.append(np.mean(all_correlations_tau))\n",
    "        kendall_spearman_scores.append(np.mean(all_correlations_spearman))\n",
    "    elif dataset_name ==\"summe\":\n",
    "        user_summarie = dataset[video_name]['user_summary']\n",
    "        pick = dataset[video_name]['picks']\n",
    "        if downsample_gt:\n",
    "            averaged_downsampled_summary = np.average(user_summarie,axis=0)[::15]\n",
    "        else:\n",
    "            averaged_downsampled_summary = np.average(user_summarie,axis=0)\n",
    "        kendall_score = kendalltau(rankdata(averaged_downsampled_summary),rankdata(score))[0]\n",
    "        spearman_score = spearmanr(averaged_downsampled_summary,score)[0]\n",
    "        kendall_spearman_scores.append(np.mean(kendall_score))\n",
    "        kendall_spearman_scores.append(np.mean(spearman_score))\n",
    "    \n",
    "    return kendall_spearman_scores\n",
    "\n",
    "# This should take an Upsampled score, or post knapsack score and then compare the correlation between them\n",
    "def correlation_with_knapsack_scores(score,video_name,dataset):\n",
    "    ''' This function first performs the knapsack processing'''\n",
    "    kendall_spearman_scores = []\n",
    "    avg_correlation_kendall = []\n",
    "    avg_correlation_spearman = []\n",
    "    user_summaries = dataset[video_name]['user_summary'][...]\n",
    "    for user_summary in user_summaries:\n",
    "        avg_correlation_kendall.append(kendalltau(-rankdata(user_summary),-rankdata(score))[0])\n",
    "        avg_correlation_spearman.append(spearmanr(user_summary,score)[0])\n",
    "    kendall_spearman_scores.append(np.mean(avg_correlation_kendall))\n",
    "    kendall_spearman_scores.append(np.mean(avg_correlation_spearman))\n",
    "\n",
    "    return kendall_spearman_scores\n",
    "\n",
    "def correlation_with_average_gt(score,video_name,dataset):\n",
    "    kendall_spearman_scores = []\n",
    "    user_summary = dataset[video_name]['gtscore'][...]\n",
    "    kendall_spearman_scores.append(kendalltau(-rankdata(user_summary),-rankdata(score))[0])\n",
    "    kendall_spearman_scores.append(spearmanr(user_summary,score)[0])\n",
    "\n",
    "    return kendall_spearman_scores\n",
    "\n",
    "\n",
    "def upsample_prediction(score,picks,video_length):\n",
    "    upsampled_pred = np.zeros(video_length)\n",
    "    for i in range(len(picks)-1):\n",
    "        upsampled_pred[picks[i]:picks[i+1]] = score[i]\n",
    "\n",
    "    return upsampled_pred \n",
    "def knapsack_wrapper_with_rating(score,test_index,dataset,dataset_name):\n",
    "    ''' This wrapper is used for scenario 2, Knapsack into evaluation of the correlation '''\n",
    "    shot_boundaries = dataset[test_index]['change_points'][...]\n",
    "    positions = dataset[test_index]['picks'][...]\n",
    "    n_frames = dataset[test_index]['n_frames'][...]\n",
    "    knapsack_pred = generate_summary_single(shot_boundaries,score,n_frames,positions)\n",
    "    return correlation_single_pred(knapsack_pred,test_index,dataset,dataset_name,False)\n",
    "\n",
    "    \n",
    "\n",
    "def upsample_wrapper(score,test_index,dataset,dataset_name):\n",
    "    '''This wrapper performs Scenario 3 post-processing, upsampling model prediction into evaluation'''\n",
    "    positions = dataset[test_index]['picks'][...]\n",
    "    n_frames = dataset[test_index]['n_frames'][...]\n",
    "    upsampled_pred = upsample_prediction(score,positions,n_frames)\n",
    "    return correlation_single_pred(upsampled_pred,test_index,dataset,dataset_name,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(run_number, config_path,save_path = 'weights'):\n",
    "    with open(config_path,'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "    \n",
    "    assert config['Model'] in model_dict.keys(), \"Model is not available, modify dictionary to include them or check spelling\"\n",
    "    dataset_name = config['split'].split(\"_\")[0]\n",
    "    split_string = config['split'].strip(dataset_name).strip('.json')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    modelclass = model_dict[config['Model']]\n",
    "    criterion = loss_dict[config['loss_function']]()\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    feature_extractor = config['feature_extractor']\n",
    "    save_name = f'{feature_extractor}_{dataset_name}{split_string}'\n",
    "    if not os.path.exists(os.path.join(save_path,save_name,dataset_name,config['Model'] )):\n",
    "        os.makedirs(os.path.join(save_path,save_name,dataset_name,config['Model'] ))\n",
    "\n",
    "\n",
    "    save_path = os.path.join(save_path,save_name,dataset_name,config['Model'])\n",
    "\n",
    "\n",
    "    params = params_dict[config['Model']][config['feature_extractor']]\n",
    "\n",
    "    if config['data_aug'] :  # Unused function for this work\n",
    "        pass\n",
    "    else:\n",
    "        data_augmentations = []\n",
    "    splits = config['total_splits'] if 'total_splits' in config.keys() else 5\n",
    "    dataset = h5py.File(os.path.join('Data',config['feature_extractor'],f'{config[\"feature_extractor\"]}_{dataset_name}.h5'))\n",
    "    print(params)\n",
    "    split_perfs_1 = [] \n",
    "    split_perfs_2 = []\n",
    "    split_perfs_3 = []\n",
    "    split_perfs_4 = []\n",
    "    for split in range(splits):\n",
    "        print(f\"Running Split:  {split+1}  for model: {config['Model']}\")\n",
    "        model = modelclass(**params)\n",
    "        batchloader = VideoData('train',config['split'],split,transforms=data_augmentations,feature_extractor=feature_extractor,trainval=True)\n",
    "        batchloader = DataLoader(batchloader,batch_size=1,shuffle=True)\n",
    "        testdata = VideoData('test',config['split'],split,feature_extractor=feature_extractor,trainval=True)\n",
    "        testloader = DataLoader(testdata,batch_size=1,shuffle=False)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"],weight_decay=config['reg'])\n",
    "        best_f1_score = -float('inf')\n",
    "        best_correlation = -float('inf')\n",
    "        best_correlation_scenario_2 = -float('inf')\n",
    "        best_correlation_scenario_3 = -float('inf')\n",
    "        best_correlation_scenario_4 = -float('inf')\n",
    "        model.to(device)\n",
    "        if 'gradnorm_clip' in config:\n",
    "            gradnorm_clip = config['gradnorm_clip']\n",
    "        else:\n",
    "            gradnorm_clip = 2\n",
    "        # Make the directory for the split if it doesn't exist \n",
    "        if not os.path.exists(os.path.join(save_path,f'split_{split+1}')):\n",
    "            os.mkdir(os.path.join(save_path,f'split_{split+1}'))\n",
    "        save_path_split = os.path.join(save_path,f'split_{split+1}')\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            total_samples = 0\n",
    "\n",
    "            for data in batchloader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                labels-=labels.min()\n",
    "                labels/=labels.max()\n",
    "                outputs = model(inputs)\n",
    "                if len(outputs.shape)>2:\n",
    "                    outputs = outputs.squeeze(-1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradnorm_clip)\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                total_samples+=1\n",
    "            epoch_loss = running_loss / len(batchloader)\n",
    "\n",
    "            model.eval()\n",
    "            test_datapoints = []\n",
    "            test_names = []\n",
    "            \n",
    "# Adding the correlation scores to have the picks from the datapoints \n",
    "            for inputs_t,names in testloader:\n",
    "                with torch.no_grad():\n",
    "                    importance_scores = model(inputs_t.to(device))\n",
    "                importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "                test_datapoints.append(importance_scores)\n",
    "                test_names.append(names[0])\n",
    "            all_scores = eval_summary(test_datapoints,dataset,test_names,dataset_name)\n",
    "\n",
    "\n",
    "\n",
    "            correlation_dict = evaluate_correlation(test_datapoints ,dataset,test_names,dataset_name)\n",
    "            scenario_2 = [knapsack_wrapper_with_rating(score,test_name,dataset,dataset_name) for score,test_name in zip(test_datapoints,test_names)]\n",
    "            scenario_3 = [upsample_wrapper(score,test_name,dataset,dataset_name) for score,test_name in zip(test_datapoints,test_names)] # Eaach of these is one \n",
    "            scenario_4 = [correlation_with_average_gt(score,test_name,dataset) for score,test_name in zip(test_datapoints,test_names)] \n",
    "            if correlation_dict['Average_Kendall']> best_correlation:    \n",
    "                print(f\"Saving epoch {epoch+1}\")\n",
    "                best_correlation = correlation_dict['Average_Kendall']\n",
    "                torch.save(model.state_dict(), os.path.join(save_path_split,f\"best_run_corr_run_{run_number}_scenario_1\" + \".pth\")) \n",
    "            if np.mean(np.array(scenario_2)[:,0]) > best_correlation_scenario_2:\n",
    "                best_correlation_scenario_2  = np.mean(np.array(scenario_2)[:,0])\n",
    "                torch.save(model.state_dict(), os.path.join(save_path_split,f\"best_run_corr_run_{run_number}_scenario_2\" + \".pth\")) \n",
    "            if np.mean(np.array(scenario_3)[:,0]) > best_correlation_scenario_3:\n",
    "                best_correlation_scenario_3  = np.mean(np.array(scenario_3)[:,0])\n",
    "                torch.save(model.state_dict(), os.path.join(save_path_split,f\"best_run_corr_run_{run_number}_scenario_3\" + \".pth\")) \n",
    "            if np.mean(np.array(scenario_4)[:,0]) > best_correlation_scenario_4:\n",
    "                best_correlation_scenario_4  = np.mean(np.array(scenario_4)[:,0])\n",
    "                torch.save(model.state_dict(), os.path.join(save_path_split,f\"best_run_corr_run_{run_number}_scenario_4\" + \".pth\")) \n",
    "            \n",
    "            \n",
    "            if np.mean(all_scores).item() > best_f1_score:\n",
    "                best_f1_score = np.mean(all_scores).item()\n",
    "                print(f\"Best F1 Score:  {epoch+1}: {best_f1_score} \")\n",
    "                #torch.save(model.state_dict(), os.path.join(save_path_split,\"best_run_f1\" + \".pth\"))\n",
    "\n",
    "        print(f'Best F1 score for split {split+1}: {best_f1_score} ')\n",
    "        print(f'Best Correlation for split {split+1}: {best_correlation} ')\n",
    "        print(f'Best Correlation of split {split+1}for Scenario 2: {best_correlation_scenario_2} ')\n",
    "        print(f'Best Correlation of split {split+1}for Scenario 3: {best_correlation_scenario_3} ')\n",
    "        print(f'Best Correlation of split {split+1}for Scenario 4: {best_correlation_scenario_4} ')\n",
    "        split_perfs_1.append(best_correlation)\n",
    "        split_perfs_2.append(best_correlation_scenario_2)\n",
    "        split_perfs_3.append(best_correlation_scenario_3)\n",
    "        split_perfs_4.append(best_correlation_scenario_4)\n",
    "    print('Completed Training')\n",
    "    return np.mean(split_perfs_1),np.mean(split_perfs_2),np.mean(split_perfs_3),np.mean(split_perfs_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 56.94992163753752 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 57.10750290541269 \n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Best F1 Score:  4: 57.44358115407444 \n",
      "Best F1 Score:  5: 57.80279393761873 \n",
      "Saving epoch 6\n",
      "Best F1 Score:  6: 58.05361547665748 \n",
      "Saving epoch 7\n",
      "Best F1 score for split 1: 58.05361547665748 \n",
      "Best Correlation for split 1: 0.1448289421207209 \n",
      "Best Correlation of split 1for Scenario 2: 0.0964294012421923 \n",
      "Best Correlation of split 1for Scenario 3: 0.144430451431043 \n",
      "Best Correlation of split 1for Scenario 4: 0.24221812053074263 \n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 58.58061635484702 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 60.32846954630952 \n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Best F1 score for split 2: 60.32846954630952 \n",
      "Best Correlation for split 2: 0.17672822147144093 \n",
      "Best Correlation of split 2for Scenario 2: 0.11149061969412326 \n",
      "Best Correlation of split 2for Scenario 3: 0.17600385490469717 \n",
      "Best Correlation of split 2for Scenario 4: 0.3236750558425553 \n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 49.96840763258693 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 50.858318912488485 \n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Saving epoch 8\n",
      "Saving epoch 10\n",
      "Best F1 score for split 3: 50.858318912488485 \n",
      "Best Correlation for split 3: 0.14730893056783598 \n",
      "Best Correlation of split 3for Scenario 2: 0.06666980940294517 \n",
      "Best Correlation of split 3for Scenario 3: 0.1468291781866367 \n",
      "Best Correlation of split 3for Scenario 4: 0.2889253351794046 \n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 61.960686102548344 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 62.3094277166381 \n",
      "Saving epoch 3\n",
      "Best F1 Score:  3: 62.61204941100185 \n",
      "Saving epoch 4\n",
      "Best F1 Score:  4: 62.726242730734874 \n",
      "Saving epoch 5\n",
      "Best F1 Score:  5: 63.0433096314362 \n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Best F1 score for split 4: 63.0433096314362 \n",
      "Best Correlation for split 4: 0.20594012564565328 \n",
      "Best Correlation of split 4for Scenario 2: 0.1414332222167515 \n",
      "Best Correlation of split 4for Scenario 3: 0.2036175858962465 \n",
      "Best Correlation of split 4for Scenario 4: 0.3434403602106716 \n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 56.657885609505584 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 57.292696350708695 \n",
      "Saving epoch 3\n",
      "Best F1 Score:  3: 58.339639983839334 \n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Best F1 Score:  13: 58.657179156847974 \n",
      "Best F1 score for split 5: 58.657179156847974 \n",
      "Best Correlation for split 5: 0.2080064617644975 \n",
      "Best Correlation of split 5for Scenario 2: 0.07428147192264847 \n",
      "Best Correlation of split 5for Scenario 3: 0.2068275934674698 \n",
      "Best Correlation of split 5for Scenario 4: 0.36133232125103937 \n",
      "Completed Training\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 58.332158770343014 \n",
      "Saving epoch 2\n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 7\n",
      "Saving epoch 8\n",
      "Best F1 score for split 1: 58.332158770343014 \n",
      "Best Correlation for split 1: 0.1329441946868357 \n",
      "Best Correlation of split 1for Scenario 2: 0.0876817823256022 \n",
      "Best Correlation of split 1for Scenario 3: 0.1335113670086091 \n",
      "Best Correlation of split 1for Scenario 4: 0.22423105653296593 \n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 57.3280172442006 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 58.72944302836017 \n",
      "Saving epoch 3\n",
      "Best F1 Score:  3: 59.15672366249979 \n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Best F1 Score:  5: 60.336908725451636 \n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Saving epoch 8\n",
      "Saving epoch 10\n",
      "Saving epoch 12\n",
      "Saving epoch 15\n",
      "Saving epoch 16\n",
      "Best F1 score for split 2: 60.336908725451636 \n",
      "Best Correlation for split 2: 0.16985772307258082 \n",
      "Best Correlation of split 2for Scenario 2: 0.10802509099515659 \n",
      "Best Correlation of split 2for Scenario 3: 0.1692130628774421 \n",
      "Best Correlation of split 2for Scenario 4: 0.3137080226982554 \n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 54.504481073256684 \n",
      "Saving epoch 2\n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Saving epoch 8\n",
      "Saving epoch 11\n",
      "Saving epoch 13\n",
      "Best F1 score for split 3: 54.504481073256684 \n",
      "Best Correlation for split 3: 0.1313114897325349 \n",
      "Best Correlation of split 3for Scenario 2: 0.0680227384836992 \n",
      "Best Correlation of split 3for Scenario 3: 0.13096987935239726 \n",
      "Best Correlation of split 3for Scenario 4: 0.25153962528508905 \n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 60.28204784248743 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 63.72058107109207 \n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Saving epoch 8\n",
      "Best F1 score for split 4: 63.72058107109207 \n",
      "Best Correlation for split 4: 0.19843549235031935 \n",
      "Best Correlation of split 4for Scenario 2: 0.1483151711376142 \n",
      "Best Correlation of split 4for Scenario 3: 0.19497741489736697 \n",
      "Best Correlation of split 4for Scenario 4: 0.3270268093547687 \n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 57.485642025711 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 58.77532051169582 \n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Best F1 Score:  4: 59.146033785102375 \n",
      "Saving epoch 5\n",
      "Best F1 Score:  5: 59.15178214449993 \n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Saving epoch 8\n",
      "Saving epoch 9\n",
      "Saving epoch 12\n",
      "Best F1 score for split 5: 59.15178214449993 \n",
      "Best Correlation for split 5: 0.20364523476263913 \n",
      "Best Correlation of split 5for Scenario 2: 0.07515144875197549 \n",
      "Best Correlation of split 5for Scenario 3: 0.20186872445322238 \n",
      "Best Correlation of split 5for Scenario 4: 0.35274641619425906 \n",
      "Completed Training\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 57.98509393450304 \n",
      "Saving epoch 2\n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Best F1 score for split 1: 57.98509393450304 \n",
      "Best Correlation for split 1: 0.1457352640412735 \n",
      "Best Correlation of split 1for Scenario 2: 0.08755134853806605 \n",
      "Best Correlation of split 1for Scenario 3: 0.14593728989690946 \n",
      "Best Correlation of split 1for Scenario 4: 0.2500263884033823 \n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 55.13475311414588 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 58.39302700364859 \n",
      "Saving epoch 3\n",
      "Best F1 Score:  3: 58.838579147114686 \n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Best F1 Score:  5: 59.498845106610304 \n",
      "Saving epoch 6\n",
      "Best F1 Score:  6: 59.69788910986809 \n",
      "Saving epoch 7\n",
      "Saving epoch 8\n",
      "Saving epoch 9\n",
      "Best F1 score for split 2: 59.69788910986809 \n",
      "Best Correlation for split 2: 0.17804903754039775 \n",
      "Best Correlation of split 2for Scenario 2: 0.11354072651802596 \n",
      "Best Correlation of split 2for Scenario 3: 0.17725851431850775 \n",
      "Best Correlation of split 2for Scenario 4: 0.331830105444194 \n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 51.207118289124026 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 52.14005725163652 \n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Best F1 Score:  32: 52.45342948151724 \n",
      "Best F1 score for split 3: 52.45342948151724 \n",
      "Best Correlation for split 3: 0.14586890463509686 \n",
      "Best Correlation of split 3for Scenario 2: 0.07417486557442086 \n",
      "Best Correlation of split 3for Scenario 3: 0.145106088112201 \n",
      "Best Correlation of split 3for Scenario 4: 0.2784260931650283 \n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 60.65542665706196 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 61.58101169913884 \n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Best F1 score for split 4: 61.58101169913884 \n",
      "Best Correlation for split 4: 0.21160835520022267 \n",
      "Best Correlation of split 4for Scenario 2: 0.14833474509001082 \n",
      "Best Correlation of split 4for Scenario 3: 0.209288449741332 \n",
      "Best Correlation of split 4for Scenario 4: 0.35478143681980523 \n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 57.16424196055696 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 57.92172038069866 \n",
      "Saving epoch 3\n",
      "Best F1 Score:  3: 57.94976190067846 \n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Best F1 Score:  5: 58.20476729908677 \n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Saving epoch 8\n",
      "Best F1 Score:  13: 58.309141085460716 \n",
      "Best F1 Score:  16: 58.507231748837945 \n",
      "Best F1 Score:  18: 58.64847848313393 \n",
      "Best F1 score for split 5: 58.64847848313393 \n",
      "Best Correlation for split 5: 0.19747990770805182 \n",
      "Best Correlation of split 5for Scenario 2: 0.07517505099934205 \n",
      "Best Correlation of split 5for Scenario 3: 0.19604683893137234 \n",
      "Best Correlation of split 5for Scenario 4: 0.34196354417560293 \n",
      "Completed Training\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 52.840821767632825 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 55.05767024105277 \n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Best F1 Score:  4: 55.43432025140722 \n",
      "Saving epoch 5\n",
      "Best F1 Score:  6: 56.42991535583603 \n",
      "Saving epoch 7\n",
      "Best F1 Score:  8: 56.60057657501071 \n",
      "Saving epoch 10\n",
      "Best F1 Score:  16: 56.95837566680037 \n",
      "Best F1 Score:  17: 57.19009264950536 \n",
      "Best F1 Score:  18: 57.60608094718119 \n",
      "Best F1 score for split 1: 57.60608094718119 \n",
      "Best Correlation for split 1: 0.13971927527352218 \n",
      "Best Correlation of split 1for Scenario 2: 0.08879962656518928 \n",
      "Best Correlation of split 1for Scenario 3: 0.13950291976252435 \n",
      "Best Correlation of split 1for Scenario 4: 0.2371393095095376 \n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 57.81191568474837 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 58.10569672107873 \n",
      "Saving epoch 3\n",
      "Best F1 Score:  3: 58.745307788523704 \n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Best F1 Score:  5: 59.7664001539372 \n",
      "Saving epoch 6\n",
      "Saving epoch 8\n",
      "Saving epoch 11\n",
      "Saving epoch 12\n",
      "Saving epoch 14\n",
      "Best F1 score for split 2: 59.7664001539372 \n",
      "Best Correlation for split 2: 0.17784842564756873 \n",
      "Best Correlation of split 2for Scenario 2: 0.13081262803690358 \n",
      "Best Correlation of split 2for Scenario 3: 0.17659565113111747 \n",
      "Best Correlation of split 2for Scenario 4: 0.3267366485181477 \n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 54.93327822112574 \n",
      "Saving epoch 2\n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Saving epoch 8\n",
      "Saving epoch 11\n",
      "Best F1 score for split 3: 54.93327822112574 \n",
      "Best Correlation for split 3: 0.16085074103839192 \n",
      "Best Correlation of split 3for Scenario 2: 0.08799940628192665 \n",
      "Best Correlation of split 3for Scenario 3: 0.16003402748780515 \n",
      "Best Correlation of split 3for Scenario 4: 0.3008555101904247 \n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 61.19195224434809 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 62.644887677289944 \n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Saving epoch 8\n",
      "Best F1 score for split 4: 62.644887677289944 \n",
      "Best Correlation for split 4: 0.20275938904609253 \n",
      "Best Correlation of split 4for Scenario 2: 0.14278529782183108 \n",
      "Best Correlation of split 4for Scenario 3: 0.20070826123871535 \n",
      "Best Correlation of split 4for Scenario 4: 0.33921373965323415 \n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 56.78145206485421 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 58.439484566831936 \n",
      "Saving epoch 3\n",
      "Best F1 Score:  3: 59.14179485690751 \n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Best F1 Score:  6: 59.80058009038974 \n",
      "Saving epoch 7\n",
      "Best F1 score for split 5: 59.80058009038974 \n",
      "Best Correlation for split 5: 0.20222974509328945 \n",
      "Best Correlation of split 5for Scenario 2: 0.07745764432738247 \n",
      "Best Correlation of split 5for Scenario 3: 0.20197153897719794 \n",
      "Best Correlation of split 5for Scenario 4: 0.3515099068167724 \n",
      "Completed Training\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 58.436757360177396 \n",
      "Saving epoch 2\n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Best F1 score for split 1: 58.436757360177396 \n",
      "Best Correlation for split 1: 0.14260388068849808 \n",
      "Best Correlation of split 1for Scenario 2: 0.09079277218951641 \n",
      "Best Correlation of split 1for Scenario 3: 0.14275192300359837 \n",
      "Best Correlation of split 1for Scenario 4: 0.24432160353193622 \n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 58.72617727791247 \n",
      "Saving epoch 2\n",
      "Saving epoch 3\n",
      "Best F1 Score:  3: 58.83574270376822 \n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Best F1 Score:  5: 59.72509647617204 \n",
      "Saving epoch 6\n",
      "Best F1 Score:  6: 60.237430414304924 \n",
      "Saving epoch 7\n",
      "Saving epoch 8\n",
      "Saving epoch 10\n",
      "Best F1 Score:  12: 60.43515143582905 \n",
      "Best F1 score for split 2: 60.43515143582905 \n",
      "Best Correlation for split 2: 0.1834847179331846 \n",
      "Best Correlation of split 2for Scenario 2: 0.1235993063917693 \n",
      "Best Correlation of split 2for Scenario 3: 0.18153191628300422 \n",
      "Best Correlation of split 2for Scenario 4: 0.34066312885217626 \n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 52.359548420783725 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 53.092833108885806 \n",
      "Saving epoch 3\n",
      "Best F1 Score:  3: 53.59745205578654 \n",
      "Saving epoch 4\n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Saving epoch 8\n",
      "Saving epoch 9\n",
      "Best F1 score for split 3: 53.59745205578654 \n",
      "Best Correlation for split 3: 0.1460814300459275 \n",
      "Best Correlation of split 3for Scenario 2: 0.06737041431802894 \n",
      "Best Correlation of split 3for Scenario 3: 0.1454864321895722 \n",
      "Best Correlation of split 3for Scenario 4: 0.27795588245361674 \n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 60.1792797481218 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 60.6994379598173 \n",
      "Saving epoch 3\n",
      "Best F1 Score:  3: 61.04195682895522 \n",
      "Saving epoch 4\n",
      "Best F1 Score:  4: 61.21534739960316 \n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Saving epoch 7\n",
      "Best F1 Score:  7: 61.457806204902454 \n",
      "Saving epoch 8\n",
      "Best F1 score for split 4: 61.457806204902454 \n",
      "Best Correlation for split 4: 0.19764882522798963 \n",
      "Best Correlation of split 4for Scenario 2: 0.13766622791301591 \n",
      "Best Correlation of split 4for Scenario 3: 0.19510748615946663 \n",
      "Best Correlation of split 4for Scenario 4: 0.32862190141550374 \n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Saving epoch 1\n",
      "Best F1 Score:  1: 57.73525496928687 \n",
      "Saving epoch 2\n",
      "Best F1 Score:  2: 59.304943475668985 \n",
      "Saving epoch 3\n",
      "Saving epoch 4\n",
      "Best F1 Score:  4: 59.35961634766727 \n",
      "Saving epoch 5\n",
      "Saving epoch 6\n",
      "Best F1 score for split 5: 59.35961634766727 \n",
      "Best Correlation for split 5: 0.2087658648077066 \n",
      "Best Correlation of split 5for Scenario 2: 0.08066560549599192 \n",
      "Best Correlation of split 5for Scenario 3: 0.20876304757092176 \n",
      "Best Correlation of split 5for Scenario 4: 0.36681080359978135 \n",
      "Completed Training\n"
     ]
    }
   ],
   "source": [
    "five_trial_scenario_1 = []\n",
    "five_trial_scenario_2 = []\n",
    "five_trial_scenario_3 = []\n",
    "five_trial_scenario_4 = []\n",
    "for i in range(5):\n",
    "    torch.manual_seed(seeds[i])\n",
    "    best_correlation,best_correlation_scenario_2,best_correlation_scenario_3,best_correlation_scenario_4  = train(i,'Configs/MLP/googlenet_tvsum_can_1.json')\n",
    "    five_trial_scenario_1.append(best_correlation)\n",
    "    five_trial_scenario_2.append(best_correlation_scenario_2)\n",
    "    five_trial_scenario_3.append(best_correlation_scenario_3)\n",
    "    five_trial_scenario_4.append(best_correlation_scenario_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean over five iterations\n",
      "0.17438962320409088\n",
      "0.1001690568893652\n",
      "0.17337357989117505\n",
      "0.30798796502515585\n"
     ]
    }
   ],
   "source": [
    "print('Mean over five iterations')\n",
    "print(np.mean(five_trial_scenario_1))\n",
    "print(np.mean(five_trial_scenario_2))\n",
    "print(np.mean(five_trial_scenario_3))\n",
    "print(np.mean(five_trial_scenario_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance over five iterations\n",
      "1.2943194708072375e-05\n",
      "8.254003040649061e-06\n",
      "1.3372578756724833e-05\n",
      "5.004370930273496e-05\n"
     ]
    }
   ],
   "source": [
    "print('variance over five iterations')\n",
    "print(np.var(five_trial_scenario_1))\n",
    "print(np.var(five_trial_scenario_2))\n",
    "print(np.var(five_trial_scenario_3))\n",
    "print(np.var(five_trial_scenario_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {'Scenario 1':five_trial_scenario_1,'Scenario 2' : five_trial_scenario_2,'Scenario 3':five_trial_scenario_3 ,'Scenario 4' : five_trial_scenario_4 }\n",
    "json.dump(results_dict,open('Results/Trial_results_Tvsum.json','w'),indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Trials over the SumMe dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_trial_scenario_1 = []\n",
    "five_trial_scenario_2 = []\n",
    "five_trial_scenario_3 = []\n",
    "five_trial_scenario_4 = []\n",
    "for i in range(5):\n",
    "    torch.manual_seed(seeds[i])\n",
    "    best_correlation,best_correlation_scenario_2,best_correlation_scenario_3,best_correlation_scenario_4  = train(i,'Configs/MLP/googlenet_summe_can_1.json')\n",
    "    five_trial_scenario_1.append(best_correlation)\n",
    "    five_trial_scenario_2.append(best_correlation_scenario_2)\n",
    "    five_trial_scenario_3.append(best_correlation_scenario_3)\n",
    "    five_trial_scenario_4.append(best_correlation_scenario_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean over five iterations\n",
      "0.08415445081748876\n",
      "0.15273205176557683\n",
      "0.08621844444669655\n",
      "0.08415445081748876\n"
     ]
    }
   ],
   "source": [
    "print('Mean over five iterations')\n",
    "print(np.mean(five_trial_scenario_1))\n",
    "print(np.mean(five_trial_scenario_2))\n",
    "print(np.mean(five_trial_scenario_3))\n",
    "print(np.mean(five_trial_scenario_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance over five iterations\n",
      "0.00039088435623092007\n",
      "1.513135116877178e-05\n",
      "0.00039205897653938156\n",
      "0.00039088435623092007\n"
     ]
    }
   ],
   "source": [
    "print('variance over five iterations')\n",
    "print(np.var(five_trial_scenario_1))\n",
    "print(np.var(five_trial_scenario_2))\n",
    "print(np.var(five_trial_scenario_3))\n",
    "print(np.var(five_trial_scenario_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {'Scenario 1':five_trial_scenario_1,'Scenario 2' : five_trial_scenario_2,'Scenario 3':five_trial_scenario_3 ,'Scenario 4' : five_trial_scenario_4 }\n",
    "json.dump(results_dict,open('Results/Trial_results_Summe.json','w'),indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training example of different scenario performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
