{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to demonstrate evaluation divergence of video summarization models \n",
    "\n",
    "This notebook demonstrates the differences in the each of the evaluation protocols followed in previous research. \n",
    "\n",
    "For this notebook, I'd like to credit the following repositories \n",
    "\n",
    "1. [DSNet](https://github.com/li-plus/DSNet)\n",
    "2. [CSTASUM](https://github.com/thswodnjs3/CSTA)\n",
    "3. [MSVA](https://github.com/TIBHannover/MSVA/tree/master)\n",
    "4. [PGLSUM](https://github.com/e-apostolidis/PGL-SUM/tree/master)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions and model loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np \n",
    "import json \n",
    "from Utils import *\n",
    "from Model import model_dict,params_dict\n",
    "import os \n",
    "import torch\n",
    "from Data import VideoData\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "seeds = [12412,31235,123123,53216,123151] # Set the seeds to ensure the results are consistent NOTE: the results are within margin of error, but not exactly obtained on different machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Should do comparisons of any pred with avg/user annotations \n",
    "def correlation_single_pred(score,video_name,dataset,dataset_name='tvsum',downsample_gt=True):\n",
    "    \"This compares the scores with a downsampled version of the ground truth, Scenario 1\"\n",
    "    kendall_spearman_scores = []\n",
    "    if dataset_name==\"tvsum\":\n",
    "        data = load_tvsum_mat('Utils//ydata-tvsum50.mat')\n",
    "        video_number = int(video_name.split('_')[1])\n",
    "        all_user_summary = data[video_number-1]['user_anno'].T\n",
    "        pick = dataset[video_name]['picks']\n",
    "        all_correlations_tau = []\n",
    "        all_correlations_spearman = []\n",
    "        for user_summary in all_user_summary:\n",
    "            if downsample_gt:\n",
    "                down_sampled_summary = (user_summary/user_summary.max())[pick] # Change this to take the picks from which a certain frame was sampled from\n",
    "            else:\n",
    "                down_sampled_summary = (user_summary/user_summary.max())\n",
    "        \n",
    "            correlation_tau = kendalltau(-rankdata(down_sampled_summary),-rankdata(score))[0]\n",
    "            correlation_spear = spearmanr(down_sampled_summary,score)[0]\n",
    "            all_correlations_tau.append(correlation_tau)\n",
    "            all_correlations_spearman.append(correlation_spear)\n",
    "        kendall_spearman_scores.append(np.mean(all_correlations_tau))\n",
    "        kendall_spearman_scores.append(np.mean(all_correlations_spearman))\n",
    "    elif dataset_name ==\"summe\":\n",
    "        user_summarie = dataset[video_name]['user_summary']\n",
    "        pick = dataset[video_name]['picks']\n",
    "        if downsample_gt:\n",
    "            averaged_downsampled_summary = np.average(user_summarie,axis=0)[::15]\n",
    "        else:\n",
    "            averaged_downsampled_summary = np.average(user_summarie,axis=0)\n",
    "        kendall_score = kendalltau(rankdata(averaged_downsampled_summary),rankdata(score))[0]\n",
    "        spearman_score = spearmanr(averaged_downsampled_summary,score)[0]\n",
    "        kendall_spearman_scores.append(np.mean(kendall_score))\n",
    "        kendall_spearman_scores.append(np.mean(spearman_score))\n",
    "    \n",
    "    return kendall_spearman_scores\n",
    "\n",
    "# This should take an Upsampled score, or post knapsack score and then compare the correlation between them\n",
    "def correlation_with_knapsack_scores(score,video_name,dataset):\n",
    "    ''' This function first performs the knapsack processing'''\n",
    "    kendall_spearman_scores = []\n",
    "    avg_correlation_kendall = []\n",
    "    avg_correlation_spearman = []\n",
    "    user_summaries = dataset[video_name]['user_summary'][...]\n",
    "    for user_summary in user_summaries:\n",
    "        avg_correlation_kendall.append(kendalltau(-rankdata(user_summary),-rankdata(score))[0])\n",
    "        avg_correlation_spearman.append(spearmanr(user_summary,score)[0])\n",
    "    kendall_spearman_scores.append(np.mean(avg_correlation_kendall))\n",
    "    kendall_spearman_scores.append(np.mean(avg_correlation_spearman))\n",
    "\n",
    "    return kendall_spearman_scores\n",
    "\n",
    "def correlation_with_average_gt(score,video_name,dataset):\n",
    "    kendall_spearman_scores = []\n",
    "    user_summary = dataset[video_name]['gtscore'][...]\n",
    "    kendall_spearman_scores.append(kendalltau(-rankdata(user_summary),-rankdata(score))[0])\n",
    "    kendall_spearman_scores.append(spearmanr(user_summary,score)[0])\n",
    "\n",
    "    return kendall_spearman_scores\n",
    "\n",
    "\n",
    "def upsample_prediction(score,picks,video_length):\n",
    "    upsampled_pred = np.zeros(video_length)\n",
    "    for i in range(len(picks)-1):\n",
    "        upsampled_pred[picks[i]:picks[i+1]] = score[i]\n",
    "\n",
    "    return upsampled_pred \n",
    "def knapsack_wrapper_with_rating(score,test_index,dataset,dataset_name):\n",
    "    ''' This wrapper is used for scenario 2, Knapsack into evaluation of the correlation '''\n",
    "    shot_boundaries = dataset[test_index]['change_points'][...]\n",
    "    positions = dataset[test_index]['picks'][...]\n",
    "    n_frames = dataset[test_index]['n_frames'][...]\n",
    "    knapsack_pred = generate_summary_single(shot_boundaries,score,n_frames,positions)\n",
    "    return correlation_single_pred(knapsack_pred,test_index,dataset,dataset_name,False)\n",
    "\n",
    "    \n",
    "\n",
    "def upsample_wrapper(score,test_index,dataset,dataset_name):\n",
    "    '''This wrapper performs Scenario 3 post-processing, upsampling model prediction into evaluation'''\n",
    "    positions = dataset[test_index]['picks'][...]\n",
    "    n_frames = dataset[test_index]['n_frames'][...]\n",
    "    upsampled_pred = upsample_prediction(score,positions,n_frames)\n",
    "    return correlation_single_pred(upsampled_pred,test_index,dataset,dataset_name,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(run_number, config_path,save_path = 'weights'):\n",
    "    with open(config_path,'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "    \n",
    "    assert config['Model'] in model_dict.keys(), \"Model is not available, modify dictionary to include them or check spelling\"\n",
    "    dataset_name = config['split'].split(\"_\")[0]\n",
    "    split_string = config['split'].strip(dataset_name).strip('.json')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    modelclass = model_dict[config['Model']]\n",
    "    criterion = loss_dict[config['loss_function']]()\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    feature_extractor = config['feature_extractor']\n",
    "    save_name = f'{feature_extractor}_{dataset_name}{split_string}'\n",
    "    if not os.path.exists(os.path.join(save_path,save_name,dataset_name,config['Model'] )):\n",
    "        os.makedirs(os.path.join(save_path,save_name,dataset_name,config['Model'] ))\n",
    "\n",
    "\n",
    "    save_path = os.path.join(save_path,save_name,dataset_name,config['Model'])\n",
    "\n",
    "\n",
    "    params = params_dict[config['Model']][config['feature_extractor']]\n",
    "\n",
    "    if config['data_aug'] :  # Unused function for this work\n",
    "        pass\n",
    "    else:\n",
    "        data_augmentations = []\n",
    "    splits = config['total_splits'] if 'total_splits' in config.keys() else 5\n",
    "    dataset = h5py.File(os.path.join('Data',config['feature_extractor'],f'{config[\"feature_extractor\"]}_{dataset_name}.h5'))\n",
    "    print(params)\n",
    "    split_perfs_1 = [] \n",
    "    split_perfs_2 = []\n",
    "    split_perfs_3 = []\n",
    "    split_perfs_4 = []\n",
    "    for split in range(splits):\n",
    "        print(f\"Running Split:  {split+1}  for model: {config['Model']}\")\n",
    "        model = modelclass(**params)\n",
    "        batchloader = VideoData('train',config['split'],split,transforms=data_augmentations,feature_extractor=feature_extractor,trainval=True)\n",
    "        batchloader = DataLoader(batchloader,batch_size=1,shuffle=True)\n",
    "        testdata = VideoData('test',config['split'],split,feature_extractor=feature_extractor,trainval=True)\n",
    "        testloader = DataLoader(testdata,batch_size=1,shuffle=False)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"],weight_decay=config['reg'])\n",
    "        best_f1_score = -float('inf')\n",
    "        best_correlation = -float('inf')\n",
    "        best_correlation_scenario_2 = -float('inf')\n",
    "        best_correlation_scenario_3 = -float('inf')\n",
    "        best_correlation_scenario_4 = -float('inf')\n",
    "        model.to(device)\n",
    "        if 'gradnorm_clip' in config:\n",
    "            gradnorm_clip = config['gradnorm_clip']\n",
    "        else:\n",
    "            gradnorm_clip = 2\n",
    "        # Make the directory for the split if it doesn't exist \n",
    "        if not os.path.exists(os.path.join(save_path,f'split_{split+1}')):\n",
    "            os.mkdir(os.path.join(save_path,f'split_{split+1}'))\n",
    "        save_path_split = os.path.join(save_path,f'split_{split+1}')\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            total_samples = 0\n",
    "\n",
    "            for data in batchloader:\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                labels-=labels.min()\n",
    "                labels/=labels.max()\n",
    "                outputs = model(inputs)\n",
    "                if len(outputs.shape)>2:\n",
    "                    outputs = outputs.squeeze(-1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradnorm_clip)\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                total_samples+=1\n",
    "            epoch_loss = running_loss / len(batchloader)\n",
    "\n",
    "            model.eval()\n",
    "            test_datapoints = []\n",
    "            test_names = []\n",
    "            \n",
    "# Adding the correlation scores to have the picks from the datapoints \n",
    "            for inputs_t,names in testloader:\n",
    "                with torch.no_grad():\n",
    "                    importance_scores = model(inputs_t.to(device))\n",
    "                importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "                test_datapoints.append(importance_scores)\n",
    "                test_names.append(names[0])\n",
    "            all_scores = eval_summary(test_datapoints,dataset,test_names,dataset_name)\n",
    "\n",
    "\n",
    "\n",
    "            correlation_dict = evaluate_correlation(test_datapoints ,dataset,test_names,dataset_name)\n",
    "            scenario_2 = [knapsack_wrapper_with_rating(score,test_name,dataset,dataset_name) for score,test_name in zip(test_datapoints,test_names)]\n",
    "            scenario_3 = [upsample_wrapper(score,test_name,dataset,dataset_name) for score,test_name in zip(test_datapoints,test_names)] # Eaach of these is one \n",
    "            scenario_4 = [correlation_with_average_gt(score,test_name,dataset) for score,test_name in zip(test_datapoints,test_names)] \n",
    "            if correlation_dict['Average_Kendall']> best_correlation:    \n",
    "                print(f\"Saving epoch {epoch+1}\")\n",
    "                best_correlation = correlation_dict['Average_Kendall']\n",
    "                torch.save(model.state_dict(), os.path.join(save_path_split,f\"best_run_corr_run_{run_number}_scenario_1\" + \".pth\")) \n",
    "            if np.mean(np.array(scenario_2)[:,0]) > best_correlation_scenario_2:\n",
    "                best_correlation_scenario_2  = np.mean(np.array(scenario_2)[:,0])\n",
    "                torch.save(model.state_dict(), os.path.join(save_path_split,f\"best_run_corr_run_{run_number}_scenario_2\" + \".pth\")) \n",
    "            if np.mean(np.array(scenario_3)[:,0]) > best_correlation_scenario_3:\n",
    "                best_correlation_scenario_3  = np.mean(np.array(scenario_3)[:,0])\n",
    "                torch.save(model.state_dict(), os.path.join(save_path_split,f\"best_run_corr_run_{run_number}_scenario_3\" + \".pth\")) \n",
    "            if np.mean(np.array(scenario_4)[:,0]) > best_correlation_scenario_4:\n",
    "                best_correlation_scenario_4  = np.mean(np.array(scenario_4)[:,0])\n",
    "                torch.save(model.state_dict(), os.path.join(save_path_split,f\"best_run_corr_run_{run_number}_scenario_4\" + \".pth\")) \n",
    "            \n",
    "            \n",
    "            if np.mean(all_scores).item() > best_f1_score:\n",
    "                best_f1_score = np.mean(all_scores).item()\n",
    "                print(f\"Best F1 Score:  {epoch+1}: {best_f1_score} \")\n",
    "                #torch.save(model.state_dict(), os.path.join(save_path_split,\"best_run_f1\" + \".pth\"))\n",
    "\n",
    "        print(f'Best F1 score for split {split+1}: {best_f1_score} ')\n",
    "        print(f'Best Correlation for split {split+1}: {best_correlation} ')\n",
    "        print(f'Best Correlation of split {split+1}for Scenario 2: {best_correlation_scenario_2} ')\n",
    "        print(f'Best Correlation of split {split+1}for Scenario 3: {best_correlation_scenario_3} ')\n",
    "        print(f'Best Correlation of split {split+1}for Scenario 4: {best_correlation_scenario_4} ')\n",
    "        split_perfs_1.append(best_correlation)\n",
    "        split_perfs_2.append(best_correlation_scenario_2)\n",
    "        split_perfs_3.append(best_correlation_scenario_3)\n",
    "        split_perfs_4.append(best_correlation_scenario_4)\n",
    "    print('Completed Training')\n",
    "    return np.mean(split_perfs_1),np.mean(split_perfs_2),np.mean(split_perfs_3),np.mean(split_perfs_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_trial_scenario_1 = []\n",
    "five_trial_scenario_2 = []\n",
    "five_trial_scenario_3 = []\n",
    "five_trial_scenario_4 = []\n",
    "for i in range(5):\n",
    "    torch.manual_seed(seeds[i])\n",
    "    best_correlation,best_correlation_scenario_2,best_correlation_scenario_3,best_correlation_scenario_4  = train(i,'Configs/MLP/googlenet_tvsum_can_1.json')\n",
    "    five_trial_scenario_1.append(best_correlation)\n",
    "    five_trial_scenario_2.append(best_correlation_scenario_2)\n",
    "    five_trial_scenario_3.append(best_correlation_scenario_3)\n",
    "    five_trial_scenario_4.append(best_correlation_scenario_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean over five iterations\n",
      "0.17438962320409088\n",
      "0.1001690568893652\n",
      "0.17337357989117505\n",
      "0.30798796502515585\n"
     ]
    }
   ],
   "source": [
    "print('Mean over five iterations')\n",
    "print(np.mean(five_trial_scenario_1))\n",
    "print(np.mean(five_trial_scenario_2))\n",
    "print(np.mean(five_trial_scenario_3))\n",
    "print(np.mean(five_trial_scenario_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance over five iterations\n",
      "1.2943194708072375e-05\n",
      "8.254003040649061e-06\n",
      "1.3372578756724833e-05\n",
      "5.004370930273496e-05\n"
     ]
    }
   ],
   "source": [
    "print('variance over five iterations')\n",
    "print(np.var(five_trial_scenario_1))\n",
    "print(np.var(five_trial_scenario_2))\n",
    "print(np.var(five_trial_scenario_3))\n",
    "print(np.var(five_trial_scenario_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {'Scenario 1':five_trial_scenario_1,'Scenario 2' : five_trial_scenario_2,'Scenario 3':five_trial_scenario_3 ,'Scenario 4' : five_trial_scenario_4 }\n",
    "json.dump(results_dict,open('Results/Trial_results_Tvsum.json','w'),indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Trials over the SumMe dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_trial_scenario_1 = []\n",
    "five_trial_scenario_2 = []\n",
    "five_trial_scenario_3 = []\n",
    "five_trial_scenario_4 = []\n",
    "for i in range(5):\n",
    "    torch.manual_seed(seeds[i])\n",
    "    best_correlation,best_correlation_scenario_2,best_correlation_scenario_3,best_correlation_scenario_4  = train(i,'Configs/MLP/googlenet_summe_can_1.json')\n",
    "    five_trial_scenario_1.append(best_correlation)\n",
    "    five_trial_scenario_2.append(best_correlation_scenario_2)\n",
    "    five_trial_scenario_3.append(best_correlation_scenario_3)\n",
    "    five_trial_scenario_4.append(best_correlation_scenario_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean over five iterations\n",
      "0.08415445081748876\n",
      "0.15273205176557683\n",
      "0.08621844444669655\n",
      "0.08415445081748876\n"
     ]
    }
   ],
   "source": [
    "print('Mean over five iterations')\n",
    "print(np.mean(five_trial_scenario_1))\n",
    "print(np.mean(five_trial_scenario_2))\n",
    "print(np.mean(five_trial_scenario_3))\n",
    "print(np.mean(five_trial_scenario_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance over five iterations\n",
      "0.00039088435623092007\n",
      "1.513135116877178e-05\n",
      "0.00039205897653938156\n",
      "0.00039088435623092007\n"
     ]
    }
   ],
   "source": [
    "print('variance over five iterations')\n",
    "print(np.var(five_trial_scenario_1))\n",
    "print(np.var(five_trial_scenario_2))\n",
    "print(np.var(five_trial_scenario_3))\n",
    "print(np.var(five_trial_scenario_4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {'Scenario 1':five_trial_scenario_1,'Scenario 2' : five_trial_scenario_2,'Scenario 3':five_trial_scenario_3 ,'Scenario 4' : five_trial_scenario_4 }\n",
    "json.dump(results_dict,open('Results/Trial_results_Summe.json','w'),indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training example of different scenario performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
