{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to retrieve the results reported in the paper "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions and model loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np \n",
    "import json \n",
    "from Utils import *\n",
    "from Model import model_dict,params_dict\n",
    "import os \n",
    "import torch\n",
    "from Data import VideoData\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "seeds = [12412,31235,123123,53216,123151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_single_pred(score,video_name,dataset,dataset_name='tvsum',downsample_gt=True):\n",
    "    \"This compares the scores with a downsampled version of the ground truth, Scenario 1\"\n",
    "    kendall_spearman_scores = []\n",
    "    if dataset_name==\"tvsum\":\n",
    "        data = load_tvsum_mat('Utils//ydata-tvsum50.mat')\n",
    "        video_number = int(video_name.split('_')[1])\n",
    "        all_user_summary = data[video_number-1]['user_anno'].T\n",
    "        pick = dataset[video_name]['picks']\n",
    "        all_correlations_tau = []\n",
    "        all_correlations_spearman = []\n",
    "        for user_summary in all_user_summary:\n",
    "            if downsample_gt:\n",
    "                down_sampled_summary = (user_summary/user_summary.max())[pick] # Change this to take the picks from which a certain frame was sampled from\n",
    "            else:\n",
    "                down_sampled_summary = (user_summary/user_summary.max())\n",
    "        \n",
    "            correlation_tau = kendalltau(-rankdata(down_sampled_summary),-rankdata(score))[0]\n",
    "            correlation_spear = spearmanr(down_sampled_summary,score)[0]\n",
    "            all_correlations_tau.append(correlation_tau)\n",
    "            all_correlations_spearman.append(correlation_spear)\n",
    "        kendall_spearman_scores.append(np.mean(all_correlations_tau))\n",
    "        kendall_spearman_scores.append(np.mean(all_correlations_spearman))\n",
    "    elif dataset_name ==\"summe\":\n",
    "        user_summarie = dataset[video_name]['user_summary']\n",
    "        pick = dataset[video_name]['picks']\n",
    "        if downsample_gt:\n",
    "            averaged_downsampled_summary = np.average(user_summarie,axis=0)[::15]\n",
    "        else:\n",
    "            averaged_downsampled_summary = np.average(user_summarie,axis=0)\n",
    "        kendall_score = kendalltau(rankdata(averaged_downsampled_summary),rankdata(score))[0]\n",
    "        spearman_score = spearmanr(averaged_downsampled_summary,score)[0]\n",
    "        kendall_spearman_scores.append(np.mean(kendall_score))\n",
    "        kendall_spearman_scores.append(np.mean(spearman_score))\n",
    "    \n",
    "    return kendall_spearman_scores\n",
    "\n",
    "# This should take an Upsampled score, or post knapsack score and then compare the correlation between them\n",
    "def correlation_with_knapsack_scores(score,video_name,dataset):\n",
    "    ''' This function first performs the knapsack processing'''\n",
    "    kendall_spearman_scores = []\n",
    "    avg_correlation_kendall = []\n",
    "    avg_correlation_spearman = []\n",
    "    user_summaries = dataset[video_name]['user_summary'][...]\n",
    "    for user_summary in user_summaries:\n",
    "        avg_correlation_kendall.append(kendalltau(-rankdata(user_summary),-rankdata(score))[0])\n",
    "        avg_correlation_spearman.append(spearmanr(user_summary,score)[0])\n",
    "    kendall_spearman_scores.append(np.mean(avg_correlation_kendall))\n",
    "    kendall_spearman_scores.append(np.mean(avg_correlation_spearman))\n",
    "\n",
    "    return kendall_spearman_scores\n",
    "\n",
    "def correlation_with_average_gt(score,video_name,dataset):\n",
    "    kendall_spearman_scores = []\n",
    "    user_summary = dataset[video_name]['gtscore'][...]\n",
    "    kendall_spearman_scores.append(kendalltau(-rankdata(user_summary),-rankdata(score))[0])\n",
    "    kendall_spearman_scores.append(spearmanr(user_summary,score)[0])\n",
    "\n",
    "    return kendall_spearman_scores\n",
    "\n",
    "\n",
    "def upsample_prediction(score,picks,video_length):\n",
    "    upsampled_pred = np.zeros(video_length)\n",
    "    for i in range(len(picks)-1):\n",
    "        upsampled_pred[picks[i]:picks[i+1]] = score[i]\n",
    "\n",
    "    return upsampled_pred \n",
    "def knapsack_wrapper_with_rating(score,test_index,dataset,dataset_name):\n",
    "    ''' This wrapper is used for scenario 2, Knapsack into evaluation of the correlation '''\n",
    "    shot_boundaries = dataset[test_index]['change_points'][...]\n",
    "    positions = dataset[test_index]['picks'][...]\n",
    "    n_frames = dataset[test_index]['n_frames'][...]\n",
    "    knapsack_pred = generate_summary_single(shot_boundaries,score,n_frames,positions)\n",
    "    return correlation_single_pred(knapsack_pred,test_index,dataset,dataset_name,False)\n",
    "\n",
    "    \n",
    "\n",
    "def upsample_wrapper(score,test_index,dataset,dataset_name):\n",
    "    '''This wrapper performs Scenario 3 post-processing, upsampling model prediction into evaluation'''\n",
    "    positions = dataset[test_index]['picks'][...]\n",
    "    n_frames = dataset[test_index]['n_frames'][...]\n",
    "    upsampled_pred = upsample_prediction(score,positions,n_frames)\n",
    "    return correlation_single_pred(upsampled_pred,test_index,dataset,dataset_name,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(run_number, config_path,save_path = 'weights'):\n",
    "    with open(config_path,'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "    assert config['Model'] in model_dict.keys(), \"Model is not available, modify dictionary to include them or check spelling\"\n",
    "    dataset_name = config['split'].split(\"_\")[0]\n",
    "    split_string = config['split'].strip(dataset_name).strip('.json')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    modelclass = model_dict[config['Model']]\n",
    "    criterion = loss_dict[config['loss_function']]()\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    feature_extractor = config['feature_extractor']\n",
    "    save_name = f'{feature_extractor}_{dataset_name}{split_string}'\n",
    "    save_path = os.path.join(save_path,save_name,dataset_name,config['Model'])\n",
    "    params = params_dict[config['Model']][config['feature_extractor']]\n",
    "\n",
    "    if config['data_aug'] :  # Unused function for this work\n",
    "        pass\n",
    "    else:\n",
    "        data_augmentations = []\n",
    "    weight_path =save_path\n",
    "    splits = config['total_splits'] if 'total_splits' in config.keys() else 5\n",
    "    dataset = h5py.File(os.path.join('Data',config['feature_extractor'],f'{config[\"feature_extractor\"]}_{dataset_name}.h5'))\n",
    "    print(params)\n",
    "    split_perfs_1 = [] \n",
    "    split_perfs_2 = []\n",
    "    split_perfs_3 = []\n",
    "    split_perfs_4 = []\n",
    "    for split in range(splits):\n",
    "        print(f\"Running Split:  {split+1}  for model: {config['Model']}\")\n",
    "        model = modelclass(**params)\n",
    "        testdata = VideoData('test',config['split'],split,feature_extractor=feature_extractor,trainval=True)\n",
    "        testloader = DataLoader(testdata,batch_size=1,shuffle=False)\n",
    "        test_datapoints = []\n",
    "        test_names = []\n",
    "        # Scenario 1 \n",
    "        weight_path_split = os.path.join(weight_path,f\"split_{split+1}\",f'best_run_corr_run_{run_number}_scenario_1.pth')  \n",
    "        model.load_state_dict(torch.load(weight_path_split,map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "# Adding the correlation scores to have the picks from the datapoints \n",
    "        for inputs_t,names in testloader:\n",
    "            with torch.no_grad():\n",
    "                importance_scores = model(inputs_t.to(device))\n",
    "            importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "            test_datapoints.append(importance_scores)\n",
    "            test_names.append(names[0])\n",
    "        correlation_dict = evaluate_correlation(test_datapoints ,dataset,test_names,dataset_name)\n",
    "        split_perfs_1.append(correlation_dict['Average_Kendall'])\n",
    "        del model   \n",
    "# Adding the correlation scores to have the picks from the datapoints \n",
    "        test_datapoints = []\n",
    "        test_names = []\n",
    "        weight_path_split = os.path.join(weight_path,f\"split_{split+1}\",f'best_run_corr_run_{run_number}_scenario_2.pth')  \n",
    "        model = modelclass(**params)\n",
    "        model.load_state_dict(torch.load(weight_path_split,map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        for inputs_t,names in testloader:\n",
    "            with torch.no_grad():\n",
    "                importance_scores = model(inputs_t.to(device))\n",
    "            importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "            test_datapoints.append(importance_scores)\n",
    "            test_names.append(names[0])\n",
    "        split_perfs_2.append(np.mean(np.array([knapsack_wrapper_with_rating(score,test_name,dataset,dataset_name) for score,test_name in zip(test_datapoints,test_names)])[:,0]))\n",
    "        del model   \n",
    "        test_datapoints = []\n",
    "        test_names = []        \n",
    "        weight_path_split = os.path.join(weight_path,f\"split_{split+1}\",f'best_run_corr_run_{run_number}_scenario_3.pth')  \n",
    "        model = modelclass(**params)\n",
    "        \n",
    "        model.load_state_dict(torch.load(weight_path_split,map_location=device))\n",
    "        model.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        for inputs_t,names in testloader:\n",
    "            with torch.no_grad():\n",
    "                importance_scores = model(inputs_t.to(device))\n",
    "            importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "            test_datapoints.append(importance_scores)\n",
    "            test_names.append(names[0])\n",
    "        del model\n",
    "        split_perfs_3.append(np.mean(np.array([upsample_wrapper(score,test_name,dataset,dataset_name) for score,test_name in zip(test_datapoints,test_names)])[:,0]))        \n",
    "        test_datapoints = []\n",
    "        test_names = []        \n",
    "        weight_path_split = os.path.join(weight_path,f\"split_{split+1}\",f'best_run_corr_run_{run_number}_scenario_4.pth')  \n",
    "        model = modelclass(**params)\n",
    "        model.load_state_dict(torch.load(weight_path_split,map_location=device))\n",
    "        model.to(device)        \n",
    "        model.eval()\n",
    "        for inputs_t,names in testloader:\n",
    "            with torch.no_grad():\n",
    "                importance_scores = model(inputs_t.to(device))\n",
    "            importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "            test_datapoints.append(importance_scores)\n",
    "            test_names.append(names[0])\n",
    "        split_perfs_4.append(np.mean(np.array([correlation_with_average_gt(score,test_name,dataset) for score,test_name in zip(test_datapoints,test_names)])[:,0]))        \n",
    "    return np.mean(split_perfs_1),np.mean(split_perfs_2),np.mean(split_perfs_3),np.mean(split_perfs_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n"
     ]
    }
   ],
   "source": [
    "five_trial_scenario_1 = []\n",
    "five_trial_scenario_2 = []\n",
    "five_trial_scenario_3 = []\n",
    "five_trial_scenario_4 = []\n",
    "for i in range(5):\n",
    "    torch.manual_seed(seeds[i])\n",
    "    best_correlation,best_correlation_scenario_2,best_correlation_scenario_3,best_correlation_scenario_4  = evaluate(i,'Configs/MLP/googlenet_tvsum_can_1.json')\n",
    "    five_trial_scenario_1.append(best_correlation)\n",
    "    five_trial_scenario_2.append(best_correlation_scenario_2)\n",
    "    five_trial_scenario_3.append(best_correlation_scenario_3)\n",
    "    five_trial_scenario_4.append(best_correlation_scenario_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean over five iterations\n",
      "0.17438962320409088\n",
      "0.1001690568893652\n",
      "0.17337357989117505\n",
      "0.30798796502515585\n"
     ]
    }
   ],
   "source": [
    "print('Mean over five iterations')\n",
    "print(np.mean(five_trial_scenario_1))\n",
    "print(np.mean(five_trial_scenario_2))\n",
    "print(np.mean(five_trial_scenario_3))\n",
    "print(np.mean(five_trial_scenario_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n"
     ]
    }
   ],
   "source": [
    "five_trial_scenario_1 = []\n",
    "five_trial_scenario_2 = []\n",
    "five_trial_scenario_3 = []\n",
    "five_trial_scenario_4 = []\n",
    "for i in range(5):\n",
    "    torch.manual_seed(seeds[i])\n",
    "    best_correlation,best_correlation_scenario_2,best_correlation_scenario_3,best_correlation_scenario_4  = evaluate(i,'Configs/MLP/googlenet_summe_can_1.json')\n",
    "    five_trial_scenario_1.append(best_correlation)\n",
    "    five_trial_scenario_2.append(best_correlation_scenario_2)\n",
    "    five_trial_scenario_3.append(best_correlation_scenario_3)\n",
    "    five_trial_scenario_4.append(best_correlation_scenario_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean over five iterations\n",
      "0.08415445081748876\n",
      "0.15273205176557683\n",
      "0.08621844444669655\n",
      "0.08415445081748876\n"
     ]
    }
   ],
   "source": [
    "print('Mean over five iterations')\n",
    "print(np.mean(five_trial_scenario_1))\n",
    "print(np.mean(five_trial_scenario_2))\n",
    "print(np.mean(five_trial_scenario_3))\n",
    "print(np.mean(five_trial_scenario_4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(seq):\n",
    "    # http://stackoverflow.com/questions/3382352/equivalent-of-numpy-argsort-in-basic-python/3383106#3383106\n",
    "    #lambda version by Tony Veijalainen\n",
    "    return [x for x,y in sorted(enumerate(seq), key = lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'summe' # Change dataset here for replication\n",
    "gt_shot_boundary = h5py.File(f'Data/googlenet/googlenet_{dataset}.h5')\n",
    "googlenet_shots = np.load(f'Reported/googlenet_shot_boundaries_{dataset}.npy',allow_pickle=True)\n",
    "resnet_shots = np.load(f'Reported/resnet_shot_boundaries_{dataset}.npy',allow_pickle=True)\n",
    "densenet_shots = np.load(f'Reported/densenet_shot_boundaries_{dataset}.npy',allow_pickle=True)\n",
    "dataset_keys = list(gt_shot_boundary.keys())\n",
    "lengths  = [(gt_shot_boundary[key]['n_frames'][...].item()) for key in list(gt_shot_boundary.keys())]\n",
    "\n",
    "indices =g(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11500576460716297\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googlenet average f1 : 0.13774717986131765\n",
      "resnet average f1 : 0.11034652746312465\n",
      "DenseNet average f1 : 0.1215005227419415\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googlenet average f1 : 0.10201714940551831\n",
      "resnet average f1 : 0.09089453203378416\n",
      "DenseNet average f1 : 0.1215005227419415\n"
     ]
    }
   ],
   "source": [
    "googlenet_shots = np.load(f'Reported/googlenet_shot_boundaries_0.8_{dataset}.npy',allow_pickle=True)\n",
    "resnet_shots = np.load(f'Reported/resnet_shot_boundaries_0.8_{dataset}.npy',allow_pickle=True)\n",
    "densenet_shots = np.load(f'Reported/densenet_shot_boundaries_0.8_{dataset}.npy',allow_pickle=True)\n",
    "dataset_keys = list(gt_shot_boundary.keys())\n",
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)\n",
    "\n",
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11989786777833027\n"
     ]
    }
   ],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))\n",
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googlenet average f1 : 0.09233794170268375\n",
      "resnet average f1 : 0.07743198392249974\n",
      "DenseNet average f1 : 0.1215005227419415\n"
     ]
    }
   ],
   "source": [
    "googlenet_shots = np.load(f'Reported/googlenet_shot_boundaries_0.6_{dataset}.npy',allow_pickle=True)\n",
    "resnet_shots = np.load(f'Reported/resnet_shot_boundaries_0.6_{dataset}.npy',allow_pickle=True)\n",
    "densenet_shots = np.load(f'Reported/densenet_shot_boundaries_0.6_{dataset}.npy',allow_pickle=True)\n",
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)\n",
    "\n",
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11345499856805104\n"
     ]
    }
   ],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))\n",
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googlenet average f1 : 0.08213614840477737\n",
      "resnet average f1 : 0.07061977140682282\n",
      "DenseNet average f1 : 0.1215005227419415\n"
     ]
    }
   ],
   "source": [
    "googlenet_shots = np.load(f'Reported/googlenet_shot_boundaries_0.4_{dataset}.npy',allow_pickle=True)\n",
    "resnet_shots = np.load(f'Reported/resnet_shot_boundaries_0.4_{dataset}.npy',allow_pickle=True)\n",
    "densenet_shots = np.load(f'Reported/densenet_shot_boundaries_0.4_{dataset}.npy',allow_pickle=True)\n",
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)\n",
    "\n",
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12695281923192953\n"
     ]
    }
   ],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))\n",
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
