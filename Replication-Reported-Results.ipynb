{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to retrieve the results reported in the paper "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions and model loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np \n",
    "import json \n",
    "from Utils import *\n",
    "from Model import model_dict,params_dict\n",
    "import os \n",
    "import torch\n",
    "from Data import VideoData\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "seeds = [12412,31235,123123,53216,123151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_single_pred(score,video_name,dataset,dataset_name='tvsum',downsample_gt=True):\n",
    "    \"This compares the scores with a downsampled version of the ground truth, Scenario 1\"\n",
    "    kendall_spearman_scores = []\n",
    "    if dataset_name==\"tvsum\":\n",
    "        data = load_tvsum_mat('Utils//ydata-tvsum50.mat')\n",
    "        video_number = int(video_name.split('_')[1])\n",
    "        all_user_summary = data[video_number-1]['user_anno'].T\n",
    "        pick = dataset[video_name]['picks']\n",
    "        all_correlations_tau = []\n",
    "        all_correlations_spearman = []\n",
    "        for user_summary in all_user_summary:\n",
    "            if downsample_gt:\n",
    "                down_sampled_summary = (user_summary/user_summary.max())[pick] # Change this to take the picks from which a certain frame was sampled from\n",
    "            else:\n",
    "                down_sampled_summary = (user_summary/user_summary.max())\n",
    "        \n",
    "            correlation_tau = kendalltau(-rankdata(down_sampled_summary),-rankdata(score))[0]\n",
    "            correlation_spear = spearmanr(down_sampled_summary,score)[0]\n",
    "            all_correlations_tau.append(correlation_tau)\n",
    "            all_correlations_spearman.append(correlation_spear)\n",
    "        kendall_spearman_scores.append(np.mean(all_correlations_tau))\n",
    "        kendall_spearman_scores.append(np.mean(all_correlations_spearman))\n",
    "    elif dataset_name ==\"summe\":\n",
    "        user_summarie = dataset[video_name]['user_summary']\n",
    "        pick = dataset[video_name]['picks']\n",
    "        if downsample_gt:\n",
    "            averaged_downsampled_summary = np.average(user_summarie,axis=0)[::15]\n",
    "        else:\n",
    "            averaged_downsampled_summary = np.average(user_summarie,axis=0)\n",
    "        kendall_score = kendalltau(rankdata(averaged_downsampled_summary),rankdata(score))[0]\n",
    "        spearman_score = spearmanr(averaged_downsampled_summary,score)[0]\n",
    "        kendall_spearman_scores.append(np.mean(kendall_score))\n",
    "        kendall_spearman_scores.append(np.mean(spearman_score))\n",
    "    \n",
    "    return kendall_spearman_scores\n",
    "\n",
    "# This should take an Upsampled score, or post knapsack score and then compare the correlation between them\n",
    "def correlation_with_knapsack_scores(score,video_name,dataset):\n",
    "    ''' This function first performs the knapsack processing'''\n",
    "    kendall_spearman_scores = []\n",
    "    avg_correlation_kendall = []\n",
    "    avg_correlation_spearman = []\n",
    "    user_summaries = dataset[video_name]['user_summary'][...]\n",
    "    for user_summary in user_summaries:\n",
    "        avg_correlation_kendall.append(kendalltau(-rankdata(user_summary),-rankdata(score))[0])\n",
    "        avg_correlation_spearman.append(spearmanr(user_summary,score)[0])\n",
    "    kendall_spearman_scores.append(np.mean(avg_correlation_kendall))\n",
    "    kendall_spearman_scores.append(np.mean(avg_correlation_spearman))\n",
    "\n",
    "    return kendall_spearman_scores\n",
    "\n",
    "def correlation_with_average_gt(score,video_name,dataset):\n",
    "    kendall_spearman_scores = []\n",
    "    user_summary = dataset[video_name]['gtscore'][...]\n",
    "    kendall_spearman_scores.append(kendalltau(-rankdata(user_summary),-rankdata(score))[0])\n",
    "    kendall_spearman_scores.append(spearmanr(user_summary,score)[0])\n",
    "\n",
    "    return kendall_spearman_scores\n",
    "\n",
    "\n",
    "def upsample_prediction(score,picks,video_length):\n",
    "    upsampled_pred = np.zeros(video_length)\n",
    "    for i in range(len(picks)-1):\n",
    "        upsampled_pred[picks[i]:picks[i+1]] = score[i]\n",
    "\n",
    "    return upsampled_pred \n",
    "def knapsack_wrapper_with_rating(score,test_index,dataset,dataset_name):\n",
    "    ''' This wrapper is used for scenario 2, Knapsack into evaluation of the correlation '''\n",
    "    shot_boundaries = dataset[test_index]['change_points'][...]\n",
    "    positions = dataset[test_index]['picks'][...]\n",
    "    n_frames = dataset[test_index]['n_frames'][...]\n",
    "    knapsack_pred = generate_summary_single(shot_boundaries,score,n_frames,positions)\n",
    "    return correlation_single_pred(knapsack_pred,test_index,dataset,dataset_name,False)\n",
    "\n",
    "    \n",
    "\n",
    "def upsample_wrapper(score,test_index,dataset,dataset_name):\n",
    "    '''This wrapper performs Scenario 3 post-processing, upsampling model prediction into evaluation'''\n",
    "    positions = dataset[test_index]['picks'][...]\n",
    "    n_frames = dataset[test_index]['n_frames'][...]\n",
    "    upsampled_pred = upsample_prediction(score,positions,n_frames)\n",
    "    return correlation_single_pred(upsampled_pred,test_index,dataset,dataset_name,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(run_number, config_path,save_path = 'weights'):\n",
    "    with open(config_path,'r') as config_file:\n",
    "        config = json.load(config_file)\n",
    "    assert config['Model'] in model_dict.keys(), \"Model is not available, modify dictionary to include them or check spelling\"\n",
    "    dataset_name = config['split'].split(\"_\")[0]\n",
    "    split_string = config['split'].strip(dataset_name).strip('.json')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    modelclass = model_dict[config['Model']]\n",
    "    criterion = loss_dict[config['loss_function']]()\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    feature_extractor = config['feature_extractor']\n",
    "    save_name = f'{feature_extractor}_{dataset_name}{split_string}'\n",
    "    save_path = os.path.join(save_path,save_name,dataset_name,config['Model'])\n",
    "    params = params_dict[config['Model']][config['feature_extractor']]\n",
    "\n",
    "    if config['data_aug'] :  # Unused function for this work\n",
    "        pass\n",
    "    else:\n",
    "        data_augmentations = []\n",
    "    weight_path =save_path\n",
    "    splits = config['total_splits'] if 'total_splits' in config.keys() else 5\n",
    "    dataset = h5py.File(os.path.join('Data',config['feature_extractor'],f'{config[\"feature_extractor\"]}_{dataset_name}.h5'))\n",
    "    print(params)\n",
    "    split_perfs_1 = [] \n",
    "    split_perfs_2 = []\n",
    "    split_perfs_3 = []\n",
    "    split_perfs_4 = []\n",
    "    for split in range(splits):\n",
    "        print(f\"Running Split:  {split+1}  for model: {config['Model']}\")\n",
    "        model = modelclass(**params)\n",
    "        testdata = VideoData('test',config['split'],split,feature_extractor=feature_extractor,trainval=True)\n",
    "        testloader = DataLoader(testdata,batch_size=1,shuffle=False)\n",
    "        test_datapoints = []\n",
    "        test_names = []\n",
    "        # Scenario 1 \n",
    "        weight_path_split = os.path.join(weight_path,f\"split_{split+1}\",f'best_run_corr_run_{run_number}_scenario_1.pth')  \n",
    "        model.load_state_dict(torch.load(weight_path_split,map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "# Adding the correlation scores to have the picks from the datapoints \n",
    "        for inputs_t,names in testloader:\n",
    "            with torch.no_grad():\n",
    "                importance_scores = model(inputs_t.to(device))\n",
    "            importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "            test_datapoints.append(importance_scores)\n",
    "            test_names.append(names[0])\n",
    "        correlation_dict = evaluate_correlation(test_datapoints ,dataset,test_names,dataset_name)\n",
    "        split_perfs_1.append(correlation_dict['Average_Kendall'])\n",
    "        del model   \n",
    "# Adding the correlation scores to have the picks from the datapoints \n",
    "        test_datapoints = []\n",
    "        test_names = []\n",
    "        weight_path_split = os.path.join(weight_path,f\"split_{split+1}\",f'best_run_corr_run_{run_number}_scenario_2.pth')  \n",
    "        model = modelclass(**params)\n",
    "        model.load_state_dict(torch.load(weight_path_split,map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        for inputs_t,names in testloader:\n",
    "            with torch.no_grad():\n",
    "                importance_scores = model(inputs_t.to(device))\n",
    "            importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "            test_datapoints.append(importance_scores)\n",
    "            test_names.append(names[0])\n",
    "        split_perfs_2.append(np.mean(np.array([knapsack_wrapper_with_rating(score,test_name,dataset,dataset_name) for score,test_name in zip(test_datapoints,test_names)])[:,0]))\n",
    "        del model   \n",
    "        test_datapoints = []\n",
    "        test_names = []        \n",
    "        weight_path_split = os.path.join(weight_path,f\"split_{split+1}\",f'best_run_corr_run_{run_number}_scenario_3.pth')  \n",
    "        model = modelclass(**params)\n",
    "        \n",
    "        model.load_state_dict(torch.load(weight_path_split,map_location=device))\n",
    "        model.to(device)\n",
    "\n",
    "        model.eval()\n",
    "        for inputs_t,names in testloader:\n",
    "            with torch.no_grad():\n",
    "                importance_scores = model(inputs_t.to(device))\n",
    "            importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "            test_datapoints.append(importance_scores)\n",
    "            test_names.append(names[0])\n",
    "        del model\n",
    "        split_perfs_3.append(np.mean(np.array([upsample_wrapper(score,test_name,dataset,dataset_name) for score,test_name in zip(test_datapoints,test_names)])[:,0]))        \n",
    "        test_datapoints = []\n",
    "        test_names = []        \n",
    "        weight_path_split = os.path.join(weight_path,f\"split_{split+1}\",f'best_run_corr_run_{run_number}_scenario_4.pth')  \n",
    "        model = modelclass(**params)\n",
    "        model.load_state_dict(torch.load(weight_path_split,map_location=device))\n",
    "        model.to(device)        \n",
    "        model.eval()\n",
    "        for inputs_t,names in testloader:\n",
    "            with torch.no_grad():\n",
    "                importance_scores = model(inputs_t.to(device))\n",
    "            importance_scores = importance_scores[0].to('cpu').tolist()\n",
    "            test_datapoints.append(importance_scores)\n",
    "            test_names.append(names[0])\n",
    "        split_perfs_4.append(np.mean(np.array([correlation_with_average_gt(score,test_name,dataset) for score,test_name in zip(test_datapoints,test_names)])[:,0]))        \n",
    "    return np.mean(split_perfs_1),np.mean(split_perfs_2),np.mean(split_perfs_3),np.mean(split_perfs_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tvsum50'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23504/3862611155.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mbest_correlation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_correlation_scenario_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_correlation_scenario_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbest_correlation_scenario_4\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Configs/MLP/googlenet_tvsum_can_1.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mfive_trial_scenario_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_correlation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfive_trial_scenario_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_correlation_scenario_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23504/3512500828.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(run_number, config_path, save_path)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mtest_datapoints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportance_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mtest_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0msplit_perfs_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mknapsack_wrapper_with_rating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_datapoints\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mweight_path_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mf\"split_{split+1}\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mf'best_run_corr_run_{run_number}_scenario_3.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23504/3512500828.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mtest_datapoints\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportance_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mtest_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0msplit_perfs_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mknapsack_wrapper_with_rating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_datapoints\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mweight_path_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mf\"split_{split+1}\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mf'best_run_corr_run_{run_number}_scenario_3.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23504/2827953971.py\u001b[0m in \u001b[0;36mknapsack_wrapper_with_rating\u001b[1;34m(score, test_index, dataset, dataset_name)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mn_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_frames'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mknapsack_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_summary_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshot_boundaries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_frames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpositions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcorrelation_single_pred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mknapsack_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23504/2827953971.py\u001b[0m in \u001b[0;36mcorrelation_single_pred\u001b[1;34m(score, video_name, dataset, dataset_name, downsample_gt)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mkendall_spearman_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"tvsum\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_tvsum_mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Utils//ydata-tvsum50.mat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mvideo_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mall_user_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvideo_number\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'user_anno'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProjMethSum\\VidSumMethods\\Utils\\summary_utils.py\u001b[0m in \u001b[0;36mload_tvsum_mat\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_tvsum_mat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhdf5storage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tvsum50'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tvsum50'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mdata_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tvsum50'"
     ]
    }
   ],
   "source": [
    "five_trial_scenario_1 = []\n",
    "five_trial_scenario_2 = []\n",
    "five_trial_scenario_3 = []\n",
    "five_trial_scenario_4 = []\n",
    "for i in range(5):\n",
    "    torch.manual_seed(seeds[i])\n",
    "    best_correlation,best_correlation_scenario_2,best_correlation_scenario_3,best_correlation_scenario_4  = evaluate(i,'Configs/MLP/googlenet_tvsum_can_1.json')\n",
    "    five_trial_scenario_1.append(best_correlation)\n",
    "    five_trial_scenario_2.append(best_correlation_scenario_2)\n",
    "    five_trial_scenario_3.append(best_correlation_scenario_3)\n",
    "    five_trial_scenario_4.append(best_correlation_scenario_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean over five iterations\n",
      "0.10813986140117286\n",
      "0.22213794167927006\n",
      "0.09152080062119303\n",
      "0.09762554758569422\n"
     ]
    }
   ],
   "source": [
    "print('Mean over five iterations')\n",
    "print(np.mean(five_trial_scenario_1))\n",
    "print(np.mean(five_trial_scenario_2))\n",
    "print(np.mean(five_trial_scenario_3))\n",
    "print(np.mean(five_trial_scenario_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "{'input_dims': 1024, 'feedforward_dims': 512}\n",
      "Running Split:  1  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  2  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  3  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  4  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n",
      "Running Split:  5  for model: MLP\n",
      "Data\\googlenet\n",
      "Data\\googlenet\\googlenet_summe.h5\n",
      "googlenet_summe.h5\n",
      "summe\n",
      "Data\\googlenet\\googlenet_tvsum.h5\n",
      "googlenet_tvsum.h5\n",
      "tvsum\n"
     ]
    }
   ],
   "source": [
    "five_trial_scenario_1 = []\n",
    "five_trial_scenario_2 = []\n",
    "five_trial_scenario_3 = []\n",
    "five_trial_scenario_4 = []\n",
    "for i in range(5):\n",
    "    torch.manual_seed(seeds[i])\n",
    "    best_correlation,best_correlation_scenario_2,best_correlation_scenario_3,best_correlation_scenario_4  = evaluate(i,'Configs/MLP/googlenet_summe_can_1.json')\n",
    "    five_trial_scenario_1.append(best_correlation)\n",
    "    five_trial_scenario_2.append(best_correlation_scenario_2)\n",
    "    five_trial_scenario_3.append(best_correlation_scenario_3)\n",
    "    five_trial_scenario_4.append(best_correlation_scenario_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean over five iterations\n",
      "0.08415445081748876\n",
      "0.15273205176557683\n",
      "0.08621844444669655\n",
      "0.08415445081748876\n"
     ]
    }
   ],
   "source": [
    "print('Mean over five iterations')\n",
    "print(np.mean(five_trial_scenario_1))\n",
    "print(np.mean(five_trial_scenario_2))\n",
    "print(np.mean(five_trial_scenario_3))\n",
    "print(np.mean(five_trial_scenario_4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(seq):\n",
    "    # http://stackoverflow.com/questions/3382352/equivalent-of-numpy-argsort-in-basic-python/3383106#3383106\n",
    "    #lambda version by Tony Veijalainen\n",
    "    return [x for x,y in sorted(enumerate(seq), key = lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_shot_boundary = h5py.File('Data/googlenet/googlenet_tvsum.h5')\n",
    "googlenet_shots = np.load('googlenet_shot_boundaries.npy',allow_pickle=True)\n",
    "resnet_shots = np.load('resnet_shot_boundaries.npy',allow_pickle=True)\n",
    "densenet_shots = np.load('densenet_shot_boundaries.npy',allow_pickle=True)\n",
    "dataset_keys = list(gt_shot_boundary.keys())\n",
    "lengths  = [(gt_shot_boundary[key]['n_frames'][...].item()) for key in list(gt_shot_boundary.keys())]\n",
    "\n",
    "indices =g(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7576484500440622\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googlenet average f1 : 0.4517452491313002\n",
      "resnet average f1 : 0.35952287214641804\n",
      "DenseNet average f1 : 0.35952287214641804\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet_shots = np.load('googlenet_shot_boundaries_0.8_summe.npy',allow_pickle=True)\n",
    "resnet_shots = np.load('resnet_shot_boundaries_vmax_0.8_summe.npy',allow_pickle=True)\n",
    "densenet_shots = np.load('densenet_shot_boundaries_0.8_summe.npy',allow_pickle=True)\n",
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)\n",
    "\n",
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))\n",
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet_shots = np.load('googlenet_shot_boundaries_0.6_summe.npy',allow_pickle=True)\n",
    "resnet_shots = np.load('resnet_shot_boundaries_vmax_0.6_summe.npy',allow_pickle=True)\n",
    "densenet_shots = np.load('densenet_shot_boundaries_0.6_summe.npy',allow_pickle=True)\n",
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)\n",
    "\n",
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))\n",
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet_shots = np.load('googlenet_shot_boundaries_0.4_summe.npy',allow_pickle=True)\n",
    "resnet_shots = np.load('resnet_shot_boundaries_vmax_0.4_summe.npy',allow_pickle=True)\n",
    "densenet_shots = np.load('densenet_shot_boundaries_0.4_summe.npy',allow_pickle=True)\n",
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)\n",
    "\n",
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))\n",
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_shot_boundary = h5py.File('Data/googlenet/googlenet_summe.h5')\n",
    "googlenet_shots = np.load('Fisher_shot_boundaries.npy',allow_pickle=True)\n",
    "Fishcher_1_f1_scores = []\n",
    "\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    Fishcher_1_f1_scores.append(f1)\n",
    "print(f'Fisher average f1 : {np.mean(Fishcher_1_f1_scores)}')\n",
    "\n",
    "gt_shot_boundary = h5py.File('Data/googlenet/googlenet_summe.h5')\n",
    "googlenet_shots = np.load('Fisher_shot_boundaries_summe_0.8.npy',allow_pickle=True)\n",
    "Fishcher_0_8_f1_scores = []\n",
    "\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    Fishcher_0_8_f1_scores.append(f1)\n",
    "print(f'Fisher 0.8 average f1 : {np.mean(Fishcher_0_8_f1_scores)}')\n",
    "Fishcher_0_6_f1_scores = []\n",
    "googlenet_shots = np.load('Fisher_shot_boundaries_summe_0.6.npy',allow_pickle=True)\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    Fishcher_0_6_f1_scores.append(f1)\n",
    "print(f'Fisher 0.6 average f1 : {np.mean(Fishcher_0_6_f1_scores)}')\n",
    "\n",
    "Fishcher_0_4_f1_scores = []\n",
    "googlenet_shots = np.load('Fisher_shot_boundaries_summe_0.4.npy',allow_pickle=True)\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    Fishcher_0_4_f1_scores.append(f1)\n",
    "print(f'Fisher 0.4 average f1 : {np.mean(Fishcher_0_4_f1_scores)}')\n",
    "\n",
    "\n",
    "\n",
    "results_dict = {'Vmax 1.0 ':np.mean(Fishcher_1_f1_scores) , 'Vmax 0.8':np.mean(Fishcher_0_8_f1_scores),'Vmax 0.6':np.mean(Fishcher_0_6_f1_scores),'Vmax 0.4':np.mean(Fishcher_0_8_f1_scores)}\n",
    "\n",
    "json.dump(results_dict,open('Results/Fisher_Shot_boundary_results.json','w'),indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
