{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to demonstrate the inconsistency in the Shot boundaries based on the feature representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "from torchvision.models import resnet50, ResNet50_Weights,googlenet,GoogLeNet_Weights\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
    "from torchvision.models import densenet121, DenseNet121_Weights\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "skimage.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Function to extract dense SIFT descriptors\n",
    "def dense_sift(img, step=10, window_size=16):\n",
    "    # Create SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "    \n",
    "    # Get keypoints on a dense grid\n",
    "    keypoints = [cv2.KeyPoint(x, y, window_size) for y in range(0, img.shape[0], step)\n",
    "                 for x in range(0, img.shape[1], step)]\n",
    "    \n",
    "    # Compute descriptors\n",
    "    _, descriptors = sift.compute(img, keypoints)\n",
    "    \n",
    "    return descriptors\n",
    "\n",
    "# Function to encode the frame with Fisher Vector\n",
    "def fisher_vector(gmm, descriptors):\n",
    "    # Compute the responsibilities\n",
    "    responsibilities = gmm.predict_proba(descriptors)\n",
    "    \n",
    "    # Compute the Fisher Vector\n",
    "    means = gmm.means_\n",
    "    covariances = gmm.covariances_\n",
    "    priors = gmm.weights_\n",
    "\n",
    "    # Initialize Fisher Vector\n",
    "    fisher_vector = np.zeros(2 * gmm.n_components * descriptors.shape[1])\n",
    "    \n",
    "    # Compute mean and covariance gradient\n",
    "    for i in range(gmm.n_components):\n",
    "        diff = descriptors - means[i]\n",
    "        fisher_vector[i * descriptors.shape[1]:(i + 1) * descriptors.shape[1]] = \\\n",
    "            np.sum(responsibilities[:, i][:, np.newaxis] * diff, axis=0) / np.sum(responsibilities[:, i])\n",
    "        fisher_vector[(gmm.n_components + i) * descriptors.shape[1]:(gmm.n_components + i + 1) * descriptors.shape[1]] = \\\n",
    "            np.sum(responsibilities[:, i][:, np.newaxis] * (diff ** 2 - covariances[i]), axis=0) / np.sum(responsibilities[:, i])\n",
    "\n",
    "    return fisher_vector\n",
    "\n",
    "# Main function to process video frames\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Step 1: Extract SIFT descriptors from video frames\n",
    "    all_descriptors = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % 5 == 0:  # Process every 5th frame\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            descriptors = dense_sift(gray_frame)\n",
    "            if descriptors is not None:\n",
    "                all_descriptors.append(descriptors)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Stack all descriptors into a single array\n",
    "    all_descriptors = np.vstack(all_descriptors) if all_descriptors else None\n",
    "\n",
    "    # Step 2: PCA reduction\n",
    "    if all_descriptors is not None:\n",
    "        pca = PCA(n_components=64)\n",
    "        reduced_descriptors = pca.fit_transform(all_descriptors)\n",
    "\n",
    "        # Step 3: Fit GMM\n",
    "        gmm = GaussianMixture(n_components=128)\n",
    "        gmm.fit(reduced_descriptors)\n",
    "\n",
    "        # Step 4: Encode each frame with Fisher Vector\n",
    "        fisher_vectors = []\n",
    "\n",
    "        for descriptors in all_descriptors:\n",
    "            reduced = pca.transform(descriptors)\n",
    "            fisher_vec = fisher_vector(gmm, reduced)\n",
    "            fisher_vectors.append(fisher_vec)\n",
    "\n",
    "        fisher_vectors = np.array(fisher_vectors)\n",
    "\n",
    "        # Output shape of Fisher Vectors\n",
    "        print(\"Shape of Fisher Vectors:\", fisher_vectors.shape)\n",
    "\n",
    "# Example usage\n",
    "process_video('path_to_your_video.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def popatov_feat_extract(video_path):\n",
    "    '''A description of the video feature extraction used by Popatov et al on Category Specific video summarization. Which is described as SIFT feature extraction, PCA and Fisher model'''\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Step 1: Extract SIFT descriptors from video frames\n",
    "    all_descriptors = []\n",
    "    frame_count = 0\n",
    "    sift = cv2.SIFT_create() # Create the sift feature extracot\n",
    "    pca = PCA(n_components=64)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if frame_count % 5 == 0:  # Process every 5th frame\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            _,descriptors = sift.detectAndCompute(frame)\n",
    "            if descriptors is not None:\n",
    "                all_descriptors.append(pca.fit_transform(descriptors)) # Apply PCA to the SIFT features\n",
    "\n",
    "        frame_count += 1\n",
    "    k = 128\n",
    "    gmm = learn_gmm(all_descriptors, n_modes=k)\n",
    "    cap.release()\n",
    "    def normalize(fisher_vector):\n",
    "        fisher_vector = (fisher_vector-np.mean(fisher_vector,axis=0))/(np.std(fisher_vector,axis=0))\n",
    "        v = np.sqrt(abs(fisher_vector)) * np.sign(fisher_vector)\n",
    "        return v / np.sqrt(np.dot(v, v))\n",
    "\n",
    "    fisher_vectors_array = np.array([normalize(fisher_vector(descriptor,gmm)) for descriptor in all_descriptors])\n",
    "    return fisher_vectors_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_vector_arr = popatov_feat_extract(f'C:\\\\Users\\\\test\\\\Project-order\\\\Videos\\\\tvsum/video_10.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpd_auto(K, ncp, vmax, desc_rate=1, **kwargs):\n",
    "    \"\"\"Main interface\n",
    "\n",
    "    Detect change points automatically selecting their number\n",
    "        K       - kernel between each pair of frames in video\n",
    "        ncp     - maximum ncp\n",
    "        vmax    - special parameter\n",
    "    Optional arguments:\n",
    "        lmin     - minimum segment length\n",
    "        lmax     - maximum segment length\n",
    "        desc_rate - rate of descriptor sampling (vmax always corresponds to 1x)\n",
    "\n",
    "    Note:\n",
    "        - cps are always calculated in subsampled coordinates irrespective to\n",
    "            desc_rate\n",
    "        - lmin and m should be in agreement\n",
    "    ---\n",
    "    Returns: (cps, costs)\n",
    "        cps   - best selected change-points\n",
    "        costs - costs for 0,1,2,...,m change-points\n",
    "\n",
    "    Memory requirement: ~ (3*N*N + N*ncp)*4 bytes ~= 16 * N^2 bytes\n",
    "    That is 1,6 Gb for the N=10000.\n",
    "    \"\"\"\n",
    "    m = ncp\n",
    "    (_, scores) = cpd_nonlin(K, m, backtrack=False, **kwargs)\n",
    "\n",
    "    N = K.shape[0]\n",
    "    N2 = N*desc_rate  # length of the video before subsampling\n",
    "\n",
    "    penalties = np.zeros(m+1)\n",
    "    # Prevent division by zero (in case of 0 changes)\n",
    "    ncp = np.arange(1, m+1)\n",
    "    penalties[1:] = (vmax*ncp/(2.0*N2))*(np.log(float(N2)/ncp)+1)\n",
    "\n",
    "    costs = scores/float(N) + penalties\n",
    "    m_best = np.argmin(costs)\n",
    "    (cps, scores2) = cpd_nonlin(K, m_best, **kwargs)\n",
    "\n",
    "    return (cps, scores2)\n",
    "\n",
    "\n",
    "#from scipy import weave\n",
    "\n",
    "def calc_scatters(K):\n",
    "    \"\"\"\n",
    "    Calculate scatter matrix:\n",
    "    scatters[i,j] = {scatter of the sequence with starting frame i and ending frame j}\n",
    "    \"\"\"\n",
    "    n = K.shape[0]\n",
    "    K1 = np.cumsum([0] + list(np.diag(K)))\n",
    "    K2 = np.zeros((n+1, n+1))\n",
    "    K2[1:, 1:] = np.cumsum(np.cumsum(K, 0), 1) # TODO: use the fact that K - symmetric\n",
    "\n",
    "    scatters = np.zeros((n, n))\n",
    "\n",
    "    diagK2 = np.diag(K2)\n",
    "\n",
    "    i = np.arange(n).reshape((-1,1))\n",
    "    j = np.arange(n).reshape((1,-1))\n",
    "    scatters = (K1[1:].reshape((1,-1))-K1[:-1].reshape((-1,1))\n",
    "                - (diagK2[1:].reshape((1,-1)) + diagK2[:-1].reshape((-1,1)) - K2[1:,:-1].T - K2[:-1,1:]) / ((j-i+1).astype(float) + (j==i-1).astype(float)))\n",
    "    scatters[j<i]=0\n",
    "    #code = r\"\"\"\n",
    "    #for (int i = 0; i < n; i++) {\n",
    "    #    for (int j = i; j < n; j++) {\n",
    "    #        scatters(i,j) = K1(j+1)-K1(i) - (K2(j+1,j+1)+K2(i,i)-K2(j+1,i)-K2(i,j+1))/(j-i+1);\n",
    "    #    }\n",
    "    #}\n",
    "    #\"\"\"\n",
    "    #weave.inline(code, ['K1','K2','scatters','n'], global_dict = \\\n",
    "    #    {'K1':K1, 'K2':K2, 'scatters':scatters, 'n':n}, type_converters=weave.converters.blitz)\n",
    "\n",
    "    return scatters\n",
    "\n",
    "def cpd_nonlin(K, ncp, lmin=1, lmax=100000, backtrack=True, verbose=True,\n",
    "    out_scatters=None):\n",
    "    \"\"\" Change point detection with dynamic programming\n",
    "    K - square kernel matrix\n",
    "    ncp - number of change points to detect (ncp >= 0)\n",
    "    lmin - minimal length of a segment\n",
    "    lmax - maximal length of a segment\n",
    "    backtrack - when False - only evaluate objective scores (to save memory)\n",
    "\n",
    "    Returns: (cps, obj)\n",
    "        cps - detected array of change points: mean is thought to be constant on [ cps[i], cps[i+1] )\n",
    "        obj_vals - values of the objective function for 0..m changepoints\n",
    "\n",
    "    \"\"\"\n",
    "    m = int(ncp)  # prevent numpy.int64\n",
    "\n",
    "    (n, n1) = K.shape\n",
    "    assert(n == n1), \"Kernel matrix awaited.\"\n",
    "\n",
    "    assert(n >= (m + 1)*lmin)\n",
    "    assert(n <= (m + 1)*lmax)\n",
    "    assert(lmax >= lmin >= 1)\n",
    "\n",
    "    if verbose:\n",
    "        #print \"n =\", n\n",
    "        print (\"Precomputing scatters...\")\n",
    "    J = calc_scatters(K)\n",
    "\n",
    "    if out_scatters != None:\n",
    "        out_scatters[0] = J\n",
    "\n",
    "    if verbose:\n",
    "        print (\"Inferring best change points...\")\n",
    "    # I[k, l] - value of the objective for k change-points and l first frames\n",
    "    I = 1e101*np.ones((m+1, n+1))\n",
    "    I[0, lmin:lmax] = J[0, lmin-1:lmax-1]\n",
    "\n",
    "    if backtrack:\n",
    "        # p[k, l] --- \"previous change\" --- best t[k] when t[k+1] equals l\n",
    "        p = np.zeros((m+1, n+1), dtype=int)\n",
    "    else:\n",
    "        p = np.zeros((1,1), dtype=int)\n",
    "\n",
    "    for k in range(1,m+1):\n",
    "        for l in range((k+1)*lmin, n+1):\n",
    "            tmin = max(k*lmin, l-lmax)\n",
    "            tmax = l-lmin+1\n",
    "            c = J[tmin:tmax,l-1].reshape(-1) + I[k-1, tmin:tmax].reshape(-1)\n",
    "            I[k,l] = np.min(c)\n",
    "            if backtrack:\n",
    "                p[k,l] = np.argmin(c)+tmin\n",
    "\n",
    "    #code = r\"\"\"\n",
    "    ##define max(x,y) ((x)>(y)?(x):(y))\n",
    "    #for (int k=1; k<m+1; k++) {\n",
    "    #    for (int l=(k+1)*lmin; l<n+1; l++) {\n",
    "    #        I(k, l) = 1e100; //nearly infinity\n",
    "    #        for (int t=max(k*lmin,l-lmax); t<l-lmin+1; t++) {\n",
    "    #            double c = I(k-1, t) + J(t, l-1);\n",
    "    #            if (c < I(k, l)) {\n",
    "    #                I(k, l) = c;\n",
    "    #                if (backtrack == 1) {\n",
    "    #                    p(k, l) = t;\n",
    "    #                }\n",
    "    #            }\n",
    "    #        }\n",
    "    #    }\n",
    "    #}\n",
    "    #\"\"\"\n",
    "\n",
    "    #weave.inline(code, ['m','n','p','I', 'J', 'lmin', 'lmax', 'backtrack'], \\\n",
    "    #    global_dict={'m':m, 'n':n, 'p':p, 'I':I, 'J':J, \\\n",
    "    #    'lmin':lmin, 'lmax':lmax, 'backtrack': int(1) if backtrack else int(0)},\n",
    "    #    type_converters=weave.converters.blitz)\n",
    "\n",
    "    # Collect change points\n",
    "    cps = np.zeros(m, dtype=int)\n",
    "\n",
    "    if backtrack:\n",
    "        cur = n\n",
    "        for k in range(m, 0, -1):\n",
    "            cps[k-1] = p[k, cur]\n",
    "            cur = cps[k-1]\n",
    "\n",
    "    scores = I[:, n].copy()\n",
    "    scores[scores > 1e99] = np.inf\n",
    "    return cps, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kts(n_frames,features,vmax=1, frame_skip = 1):\n",
    "      \"\"\" Receives the frame features from the CNN to do the Shot division based on KTS #TODO need to see how exactly this functions\n",
    "      \"\"\"\n",
    "      seq_len = len(features)\n",
    "      picks = np.arange(0, seq_len) * frame_skip\n",
    "\n",
    "      # compute change points using KTS\n",
    "      kernel = np.matmul(features, features.T)\n",
    "      change_points, _ = cpd_auto(kernel, seq_len - 1, vmax, verbose=False)\n",
    "      change_points *= frame_skip\n",
    "      change_points = np.hstack((0, change_points, n_frames))\n",
    "      begin_frames = change_points[:-1]\n",
    "      end_frames = change_points[1:]\n",
    "      change_points = np.vstack((begin_frames, end_frames - 1)).T\n",
    "\n",
    "      n_frame_per_seg = end_frames - begin_frames\n",
    "      return change_points, n_frame_per_seg, picks\n",
    "\n",
    "\n",
    "class THWC_to_CTHW(torch.nn.Module):\n",
    "    def forward(self, data):\n",
    "        # Do some transformations\n",
    "        return data.permute(3, 0, 1, 2)\n",
    "class PreProcessorVidSum(object):\n",
    "    def __init__(self,feature_extractor,target_downsample=2,shot_aware = True):\n",
    "        self.target_downsample = target_downsample\n",
    "        self.feature_extractor = feature_extractor # TODO add support for GPU\n",
    "        self.shot_aware = shot_aware\n",
    "    def run(self,video_path,shot_boundaries = []):\n",
    "        ''' This is using the shot boundaries from the h5 datasets to frames to pick the selected, so it returns all the frames features and the selected ones\n",
    "        '''\n",
    "        shot_boundaries = np.array(shot_boundaries).astype(int)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(frame_rate)\n",
    "        print(total_frames)\n",
    "        downsample_target = frame_rate//self.target_downsample if self.target_downsample!=0 else 1\n",
    "        picked_frames = np.arange(0,total_frames,downsample_target )\n",
    "        selected_frames = np.union1d(shot_boundaries,picked_frames)\n",
    "        print(len(selected_frames))\n",
    "        if selected_frames[-1]>total_frames-1: selected_frames[-1]=total_frames-1\n",
    "        print(selected_frames[-1],selected_frames[-2])\n",
    "        all_frames = []\n",
    "        for sub_frame in tqdm(selected_frames):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES,sub_frame)\n",
    "            ret,frame = cap.read()\n",
    "            if not ret:\n",
    "                print(f\"Error reading frame at index {sub_frame}\")\n",
    "                continue\n",
    "            all_frames.append(self.feature_extractor.run(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))).numpy())\n",
    "        cap.release()\n",
    "        return all_frames, selected_frames\n",
    "\n",
    "class FeatureExtractor():\n",
    "    def __init__(self,model,transforms):\n",
    "        self.model = model\n",
    "        self.transforms = transforms # Transforms should act like one function, otherwise, one should do this outside and pass identity through this transform\n",
    "\n",
    "    def run(self,input):\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # The model has to be in eval mode and on the GPU/CPU set outside.\n",
    "        with torch.no_grad():\n",
    "            return self.model(self.transforms(input).unsqueeze(0).to(device)).squeeze().to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(seq):\n",
    "    # http://stackoverflow.com/questions/3382352/equivalent-of-numpy-argsort-in-basic-python/3383106#3383106\n",
    "    #lambda version by Tony Veijalainen\n",
    "    return [x for x,y in sorted(enumerate(seq), key = lambda x: x[1])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsum_dataset = h5py.File('Data/googlenet/googlenet_tvsum.h5')\n",
    "\n",
    "lengths  = [(tvsum_dataset[key]['n_frames'][...].item()) for key in list(tvsum_dataset.keys())]\n",
    "indices =g(lengths)\n",
    "dataset_keys = list(tvsum_dataset.keys())\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Googlenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = googlenet(weights = GoogLeNet_Weights.IMAGENET1K_V1)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "preprocess = ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
    "\n",
    "submodel = nn.Sequential(*list(model.children())[:-2]).to(device).eval()\n",
    "feature_extractor = FeatureExtractor(submodel,preprocess)\n",
    "preprocesser_sum = PreProcessorVidSum(feature_extractor,target_downsample=0)\n",
    "dataset_features = []\n",
    "for i in range(10):\n",
    "    index = indices[i]\n",
    "    video_path = f'C:\\\\Users\\\\test\\\\Project-order\\\\Videos\\\\tvsum/{dataset_keys[index]}.mp4'\n",
    "    features,_ = preprocesser_sum.run(video_path)\n",
    "    dataset_features.append(features)\n",
    "\n",
    "\n",
    "\n",
    "np.save('GoogleNet_Features_tvsum.npy',np.array(dataset_features, dtype=object), allow_pickle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5file = tvsum_dataset\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "submodel = nn.Sequential(*list(model.children())[:-1])\n",
    "submodel.eval().to(device)\n",
    "feature_extractor = FeatureExtractor(submodel,preprocess)\n",
    "preprocesser_sum = PreProcessorVidSum(feature_extractor,target_downsample=0)\n",
    "dataset_features = []\n",
    "for i in range(10):\n",
    "    index = indices[i]\n",
    "    video_path = f'C:\\\\Users\\\\test\\\\Project-order\\\\Videos\\\\tvsum/{dataset_keys[index]}.mp4'\n",
    "    features,_ = preprocesser_sum.run(video_path)\n",
    "    dataset_features.append(features)\n",
    "\n",
    "\n",
    "\n",
    "np.save('Resnet_Features_tvsum.npy',np.array(dataset_features, dtype=object), allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = densenet121(weights =DenseNet121_Weights.IMAGENET1K_V1)\n",
    "submodel = nn.Sequential(*list(model.children())[:-1],nn.AdaptiveAvgPool2d(1)).to('cuda')\n",
    "\n",
    "processed_dataset = 'densnet'\n",
    "feature_extractor = FeatureExtractor(submodel,preprocess)\n",
    "preprocesser_sum = PreProcessorVidSum(feature_extractor,target_downsample=0)\n",
    "dataset_features = []\n",
    "for i in range(10):\n",
    "    index = indices[i]\n",
    "    video_path = f'C:\\\\Users\\\\test\\\\Project-order\\\\Videos\\\\tvsum/{dataset_keys[index]}.mp4'\n",
    "    features,_ = preprocesser_sum.run(video_path)\n",
    "    dataset_features.append(features)\n",
    "\n",
    "\n",
    "\n",
    "np.save('Densnet_Features_tvsum.npy',np.array(dataset_features, dtype=object), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of Shot Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet_features= np.load('GoogleNet_Features_tvsum.npy',allow_pickle = True)\n",
    "resnet_features = np.load('Resnet_Features_tvsum.npy',allow_pickle = True)\n",
    "densenet_features = np.load('Densnet_Features_tvsum.npy',allow_pickle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "shot_boundary = []\n",
    "\n",
    "for feature in resnet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature))\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('resnet_shot_boundaries.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)\n",
    "\n",
    "for feature in googlenet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature))\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('googlenet_shot_boundaries.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)\n",
    "\n",
    "for feature in densenet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature))\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('densenet_shot_boundaries.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "shot_boundary = []\n",
    "\n",
    "for feature in resnet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature),vmax=0.8)\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('resnet_shot_boundaries_vmax_0.8.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)\n",
    "\n",
    "for feature in googlenet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature),vmax=0.8)\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('googlenet_shot_boundaries_0.8.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)\n",
    "\n",
    "for feature in densenet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature),vmax=0.8)\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('densenet_shot_boundaries_0.8.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "shot_boundary = []\n",
    "\n",
    "for feature in resnet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature),vmax=0.6)\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('resnet_shot_boundaries_vmax_0.6.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)\n",
    "\n",
    "for feature in googlenet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature),vmax=0.6)\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('googlenet_shot_boundaries_0.6.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)\n",
    "\n",
    "for feature in densenet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature),vmax=0.6)\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('densenet_shot_boundaries_0.6.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_boundary = []\n",
    "\n",
    "for feature in resnet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature),vmax=0.4)\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('resnet_shot_boundaries_vmax_0.4.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)\n",
    "shot_boundary = []\n",
    "\n",
    "for feature in googlenet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature),vmax=0.4)\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('googlenet_shot_boundaries_0.4.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)\n",
    "shot_boundary = []\n",
    "for feature in densenet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature),vmax=0.4)\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('densenet_shot_boundaries_0.4.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fishers_features= np.load('Fishers_Features_tvsum.npy',allow_pickle = True)\n",
    "\n",
    "shot_boundary = []\n",
    "for feature in resnet_features:\n",
    "    feature = [feat/LA.norm(feat) for feat in feature]\n",
    "    n_frames = len(feature)\n",
    "    change_points,_ ,_= kts(n_frames,np.array(feature),vmax=1.0)\n",
    "    shot_boundary.append(change_points)\n",
    "np.save('Fisher_shot_boundaries.npy',np.array(shot_boundary, dtype=object), allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_boundaries, predicted_boundaries):\n",
    "    TP = len(set(true_boundaries) & set(predicted_boundaries))\n",
    "    FP = len(set(predicted_boundaries) - set(true_boundaries))\n",
    "    FN = len(set(true_boundaries) - set(predicted_boundaries))\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "  \n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_shot_boundary = h5py.File('Data/googlenet/googlenet_tvsum.h5')\n",
    "googlenet_shots = np.load('googlenet_shot_boundaries.npy',allow_pickle=True)[10:20]\n",
    "resnet_shots = np.load('resnet_shot_boundaries.npy',allow_pickle=True)\n",
    "densenet_shots = np.load('densenet_shot_boundaries.npy',allow_pickle=True)[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(densenet_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27122742499849734\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googlenet average f1 : 0.4517452491313002\n",
      "resnet average f1 : 0.35952287214641804\n",
      "DenseNet average f1 : 0.08209442409442409\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.8 Vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 2499]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "densenet_shots[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googlenet average f1 : 0.39347434440659274\n",
      "resnet average f1 : 0.3168795153733152\n",
      "DenseNet average f1 : 0.08209442409442409\n"
     ]
    }
   ],
   "source": [
    "googlenet_shots = np.load('googlenet_shot_boundaries_0.8.npy',allow_pickle=True)[10:20]\n",
    "resnet_shots = np.load('resnet_shot_boundaries_vmax_0.8.npy',allow_pickle=True)\n",
    "densenet_shots = np.load('densenet_shot_boundaries_0.8.npy',allow_pickle=True)[20:30]\n",
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)\n",
    "\n",
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))\n",
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googlenet average f1 : 0.32693164649341244\n",
      "resnet average f1 : 0.2633254972947965\n",
      "DenseNet average f1 : 0.08209442409442409\n"
     ]
    }
   ],
   "source": [
    "googlenet_shots = np.load('googlenet_shot_boundaries_0.6.npy',allow_pickle=True)[10:20]\n",
    "resnet_shots = np.load('resnet_shot_boundaries_vmax_0.6.npy',allow_pickle=True)\n",
    "densenet_shots = np.load('densenet_shot_boundaries_0.6.npy',allow_pickle=True)[20:30]\n",
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)\n",
    "\n",
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23142867232486375\n"
     ]
    }
   ],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))\n",
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googlenet average f1 : 0.2372799281259132\n",
      "resnet average f1 : 0.19722524250195686\n",
      "DenseNet average f1 : 0.08209442409442409\n"
     ]
    }
   ],
   "source": [
    "googlenet_shots = np.load('googlenet_shot_boundaries_0.4.npy',allow_pickle=True)\n",
    "resnet_shots = np.load('resnet_shot_boundaries_vmax_0.4.npy',allow_pickle=True)\n",
    "densenet_shots = np.load('densenet_shot_boundaries_0.4.npy',allow_pickle=True)\n",
    "googlenet_f1_scores = []\n",
    "resnet_f1_scores = []\n",
    "densenet_f1_scores = []\n",
    "for i,index in enumerate(indices[:10]):\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),googlenet_shots[i].flatten())\n",
    "    googlenet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),resnet_shots[i].flatten())\n",
    "    resnet_f1_scores.append(f1)\n",
    "    precison, recall, f1 = calculate_metrics(gt_shot_boundary[dataset_keys[index]]['change_points'][...].flatten(),densenet_shots[i].flatten())\n",
    "    densenet_f1_scores.append(f1)\n",
    "\n",
    "\n",
    "print(f'Googlenet average f1 : {np.mean(googlenet_f1_scores)}')\n",
    "print(f'resnet average f1 : {np.mean(resnet_f1_scores)}')\n",
    "print(f'DenseNet average f1 : {np.mean(densenet_f1_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21228872323701808\n"
     ]
    }
   ],
   "source": [
    "perfs_avg = []\n",
    "for i in range(10):\n",
    "    _,_,f1_goog_res = calculate_metrics(resnet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    _,_,f1_res_dense= calculate_metrics(densenet_shots[i].flatten(),resnet_shots[i].flatten())\n",
    "    _,_,f1_dens_gog= calculate_metrics(densenet_shots[i].flatten(),googlenet_shots[i].flatten())\n",
    "    perfs_avg.append( np.mean([f1_goog_res,f1_res_dense]))\n",
    "print(np.mean(perfs_avg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
