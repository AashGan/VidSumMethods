{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to demonstrate evaluation divergence of video summarization models \n",
    "\n",
    "Rough approach: take trained model output, see if trained model output diverges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions and model loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np \n",
    "import json \n",
    "from Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Should do comparisons of any pred with avg/user annotations \n",
    "def correlation_single_pred(score,video_name,dataset,dataset_name='tvsum',downsample_gt=True):\n",
    "    \"This compares the scores with a downsampled version of the ground truth\"\n",
    "    kendall_spearman_scores = []\n",
    "    if dataset_name==\"tvsum\":\n",
    "        data = load_tvsum_mat('Utils//ydata-tvsum50.mat')\n",
    "        video_number = int(video_name.split('_')[1])\n",
    "        all_user_summary = dataset[video_number-1]['user_anno'].T\n",
    "        pick = dataset[video_name]['picks']\n",
    "        all_correlations_tau = []\n",
    "        all_correlations_spearman = []\n",
    "        for user_summary in all_user_summary:\n",
    "            if downsample_gt:\n",
    "                down_sampled_summary = (user_summary/user_summary.max())[pick] # Change this to take the picks from which a certain frame was sampled from\n",
    "            else:\n",
    "                down_sampled_summary = (user_summary/user_summary.max())\n",
    "            correlation_tau = kendalltau(-rankdata(down_sampled_summary),-rankdata(score))[0]\n",
    "            correlation_spear = spearmanr(down_sampled_summary,score)[0]\n",
    "            all_correlations_tau.append(correlation_tau)\n",
    "            all_correlations_spearman.append(correlation_spear)\n",
    "        kendall_spearman_scores.append(np.mean(all_correlations_tau))\n",
    "        kendall_spearman_scores.append(np.mean(all_correlations_spearman))\n",
    "    elif dataset_name ==\"summe\":\n",
    "        user_summarie = dataset[video_name]['user_summary']\n",
    "        if downsample_gt:\n",
    "            averaged_downsampled_summary = np.average(user_summarie,axis=0)[pick]\n",
    "        else:\n",
    "            averaged_downsampled_summary = np.average(user_summarie,axis=0)\n",
    "        kendall_score = kendalltau(rankdata(averaged_downsampled_summary),rankdata(score))[0]\n",
    "        spearman_score = spearmanr(averaged_downsampled_summary,score)[0]\n",
    "        kendall_spearman_scores.append(np.mean(kendall_score))\n",
    "        kendall_spearman_scores.append(np.mean(spearman_score))\n",
    "    \n",
    "    return kendall_spearman_scores\n",
    "\n",
    "# This should take an Upsampled score, or post knapsack score and then compare the correlation between them\n",
    "def correlation_with_knapsack_scores(score,video_name,dataset):\n",
    "    kendall_spearman_scores = []\n",
    "    avg_correlation_kendall = []\n",
    "    avg_correlation_spearman = []\n",
    "    user_summaries = dataset[video_name]['user_summary'][...]\n",
    "    for user_summary in user_summaries:\n",
    "        avg_correlation_kendall.append(kendalltau(-rankdata(user_summary),-rankdata(score))[0])\n",
    "        avg_correlation_spearman.append(spearmanr(user_summary,score)[0])\n",
    "    kendall_spearman_scores.append(np.mean(avg_correlation_kendall))\n",
    "    kendall_spearman_scores.append(np.mean(avg_correlation_spearman))\n",
    "\n",
    "    return kendall_spearman_scores\n",
    "\n",
    "def upsample_prediction(score,picks,video_length):\n",
    "    upsampled_pred = np.zeros(video_length)\n",
    "    for i in range(len(picks)-1):\n",
    "        upsampled_pred[picks[i]:picks[i+1]] = score[i]\n",
    "\n",
    "    return upsampled_pred \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing before main experiment      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = h5py.File('Data/original/googlenet_summe.h5')\n",
    "test_split = json.load(open('Splits/tvsum_can_1.json'))\n",
    "test_split_index = test_split[0]['test_keys']\n",
    "\n",
    "model_outputs = json.load(open('SensitivityExpt/Attention/Output/outputs.json'))\n",
    "scenario_1 =[]\n",
    "scenario_2 = []\n",
    "scenario_3 = []\n",
    "scenario_4  = []\n",
    "for test_index in test_split_index:\n",
    "    shot_boundaries = dataset[test_index]['change_points'][...]\n",
    "    scores = model_outputs[test_index] \n",
    "    positions = dataset[test_index]['picks'][...]\n",
    "    n_frames = dataset[test_index]['n_frames'][...]\n",
    "    user_summaries = dataset[test_index]['user_summaries'][...]\n",
    "    post_knapsack_pred = generate_summary_single(shot_boundaries,scores,n_frames,positions)\n",
    "    upsampled_pred = upsample_prediction(scores,positions,n_frames)\n",
    "    print('Evaluating Correlation with Downsampled prediction and 0 to 1 scores')\n",
    "    correlation_scores = correlation_single_pred(scores,test_index,dataset,'tvsum')\n",
    "    print(f'Kendall scores:  {correlation_scores[0]}')\n",
    "    scenario_1.append(correlation_scores)\n",
    "    print('Evaluating scores with Post knapsack Processing and 0 to 1 scores')\n",
    "    correlation_scores = correlation_single_pred(post_knapsack_pred,test_index,dataset,'tvsum',False)\n",
    "    print(f'Kendall scores:  {correlation_scores[0]}')\n",
    "    scenario_2.append(correlation_scores)\n",
    "    print('Evaluating upsampled predictions with 0/1 selection annotations')\n",
    "    correlation_scores = correlation_with_knapsack_scores(post_knapsack_pred,test_index,dataset)\n",
    "    print(f'Kendall scores:  {correlation_scores[0]}')\n",
    "    scenario_3.append(correlation_scores)\n",
    "    print('Evaluating scores with Post knapsack Processing  with 0/1 selection annotations ')\n",
    "    correlation_scores = correlation_with_knapsack_scores(upsampled_pred,test_index,dataset)\n",
    "    print(f'Kendall scores:  {correlation_scores[0]}')\n",
    "    scenario_4.append(correlation_scores)\n",
    "    \n",
    "\n",
    "    # Comparing correlation between the scores and the predictions directly \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scenario 1 scores : {np.mean(scenario_1)}')\n",
    "print(f'Scenario 2 scores : {np.mean(scenario_2)}')\n",
    "print(f'Scenario 3 scores : {np.mean(scenario_3)}')\n",
    "print(f'Scenario 4 scores : {np.mean(scenario_4)}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
